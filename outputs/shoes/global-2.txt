=> world size: 8
=> rank: 7
=> dist_url: tcp://127.0.0.1:22223
=> world size:=> local_rank:  87

=> rank: 3
=> dist_url: tcp://127.0.0.1:22223
=> local_rank: 3
=> world size:=> world size:  88

=> rank:=> rank:  50

=> dist_url:=> dist_url:  tcp://127.0.0.1:22223tcp://127.0.0.1:22223

=> local_rank:=> local_rank: => world size: 5=> world size: 0
 8
8

=> rank:=> rank:  16

=> dist_url:=> dist_url:  tcp://127.0.0.1:22223tcp://127.0.0.1:22223

=> local_rank:=> local_rank:  16

=> world size: 8
=> rank: 2
=> dist_url: tcp://127.0.0.1:22223
=> local_rank: 2
=> world size: 8
=> rank: 4
=> dist_url: tcp://127.0.0.1:22223
=> local_rank: 4
Shoes
Shoes
Shoes
Shoes
Shoes
Shoes
Shoes
Shoes
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
Changed CLIP Model, designed for enhanced text
CLIP model loaded successfully
Begin to train
Train Epoch: [0][0/17]	Loss 4.1687 (4.1687)	LossFusion 4.1687 (4.1687)	
Train Epoch: [0][16/17]	Loss 1.5529 (2.6269)	LossFusion 1.5529 (2.6269)	
R@10:  50.82339644432068     R@50:  77.96706557273865
Best Mean Now:  64.39523 ******************************
Train Epoch: [1][0/17]	Loss 1.9514 (1.9514)	LossFusion 1.9514 (1.9514)	
Train Epoch: [1][16/17]	Loss 1.2274 (1.5833)	LossFusion 1.2274 (1.5833)	
R@10:  52.92447209358215     R@50:  79.38671112060547
Best Mean Now:  66.15559 ******************************
Train Epoch: [2][0/17]	Loss 1.6304 (1.6304)	LossFusion 1.6304 (1.6304)	
Train Epoch: [2][16/17]	Loss 1.1159 (1.3319)	LossFusion 1.1159 (1.3319)	
R@10:  54.116976261138916     R@50:  80.01135587692261
Best Mean Now:  67.06417 ******************************
Train Epoch: [3][0/17]	Loss 1.5202 (1.5202)	LossFusion 1.5202 (1.5202)	
Train Epoch: [3][16/17]	Loss 1.0540 (1.2676)	LossFusion 1.0540 (1.2676)	
R@10:  54.17376756668091     R@50:  80.12492656707764
Best Mean Now:  67.14935 ******************************
Train Epoch: [4][0/17]	Loss 1.4125 (1.4125)	LossFusion 1.4125 (1.4125)	
Train Epoch: [4][16/17]	Loss 0.9755 (1.1809)	LossFusion 0.9755 (1.1809)	
R@10:  54.17376756668091     R@50:  80.4656445980072
Best Mean Now:  67.31971 ******************************
Train Epoch: [5][0/17]	Loss 1.3017 (1.3017)	LossFusion 1.3017 (1.3017)	
Train Epoch: [5][16/17]	Loss 0.9418 (1.1341)	LossFusion 0.9418 (1.1341)	
R@10:  54.514479637145996     R@50:  81.20386004447937
Best Mean Now:  67.85917 ******************************
Train Epoch: [6][0/17]	Loss 1.2002 (1.2002)	LossFusion 1.2002 (1.2002)	
Train Epoch: [6][16/17]	Loss 0.8728 (1.0839)	LossFusion 0.8728 (1.0839)	
R@10:  54.628050327301025     R@50:  81.09028935432434
Mean Now:  67.85916984081268  Best Mean Before:  67.85917 --------------------
Train Epoch: [7][0/17]	Loss 1.1873 (1.1873)	LossFusion 1.1873 (1.1873)	
Train Epoch: [7][16/17]	Loss 0.8510 (1.0417)	LossFusion 0.8510 (1.0417)	
R@10:  55.08233904838562     R@50:  81.09028935432434
Best Mean Now:  68.08631 ******************************
Train Epoch: [8][0/17]	Loss 1.1949 (1.1949)	LossFusion 1.1949 (1.1949)	
Train Epoch: [8][16/17]	Loss 0.8018 (1.0231)	LossFusion 0.8018 (1.0231)	
R@10:  54.40090894699097     R@50:  80.9199333190918
Mean Now:  67.66042113304138  Best Mean Before:  68.08631 --------------------
Train Epoch: [9][0/17]	Loss 1.1174 (1.1174)	LossFusion 1.1174 (1.1174)	
Train Epoch: [9][16/17]	Loss 0.7365 (0.9569)	LossFusion 0.7365 (0.9569)	
R@10:  53.83304953575134     R@50:  81.03350400924683
Mean Now:  67.43327677249908  Best Mean Before:  68.08631 --------------------
Train Epoch: [10][0/17]	Loss 1.0692 (1.0692)	LossFusion 1.0692 (1.0692)	
Train Epoch: [10][16/17]	Loss 0.6990 (0.9297)	LossFusion 0.6990 (0.9297)	
R@10:  54.514479637145996     R@50:  81.09028935432434
Mean Now:  67.80238449573517  Best Mean Before:  68.08631 --------------------
Train Epoch: [11][0/17]	Loss 1.1113 (1.1113)	LossFusion 1.1113 (1.1113)	
Train Epoch: [11][16/17]	Loss 0.6973 (0.9071)	LossFusion 0.6973 (0.9071)	
R@10:  55.19590973854065     R@50:  81.3174307346344
Best Mean Now:  68.25667 ******************************
Train Epoch: [12][0/17]	Loss 1.0554 (1.0554)	LossFusion 1.0554 (1.0554)	
Train Epoch: [12][16/17]	Loss 0.6706 (0.8469)	LossFusion 0.6706 (0.8469)	
R@10:  54.96876835823059     R@50:  82.05565214157104
Best Mean Now:  68.51221 ******************************
Train Epoch: [13][0/17]	Loss 1.0308 (1.0308)	LossFusion 1.0308 (1.0308)	
Train Epoch: [13][16/17]	Loss 0.6361 (0.8167)	LossFusion 0.6361 (0.8167)	
R@10:  54.85519766807556     R@50:  81.3174307346344
Mean Now:  68.08631420135498  Best Mean Before:  68.51221 --------------------
Train Epoch: [14][0/17]	Loss 0.9833 (0.9833)	LossFusion 0.9833 (0.9833)	
Train Epoch: [14][16/17]	Loss 0.5966 (0.7858)	LossFusion 0.5966 (0.7858)	
R@10:  54.28733825683594     R@50:  81.26064538955688
Mean Now:  67.77399182319641  Best Mean Before:  68.51221 --------------------
Train Epoch: [15][0/17]	Loss 0.8698 (0.8698)	LossFusion 0.8698 (0.8698)	
Train Epoch: [15][16/17]	Loss 0.6467 (0.7402)	LossFusion 0.6467 (0.7402)	
R@10:  54.40090894699097     R@50:  81.3174307346344
Mean Now:  67.85916984081268  Best Mean Before:  68.51221 --------------------
Train Epoch: [16][0/17]	Loss 0.8981 (0.8981)	LossFusion 0.8981 (0.8981)	
Train Epoch: [16][16/17]	Loss 0.4654 (0.7004)	LossFusion 0.4654 (0.7004)	
R@10:  55.08233904838562     R@50:  81.4310073852539
Mean Now:  68.25667321681976  Best Mean Before:  68.51221 --------------------
Train Epoch: [17][0/17]	Loss 0.8331 (0.8331)	LossFusion 0.8331 (0.8331)	
Train Epoch: [17][16/17]	Loss 0.4706 (0.6907)	LossFusion 0.4706 (0.6907)	
R@10:  54.40090894699097     R@50:  81.09028935432434
Mean Now:  67.74559915065765  Best Mean Before:  68.51221 --------------------
Train Epoch: [18][0/17]	Loss 0.8350 (0.8350)	LossFusion 0.8350 (0.8350)	
Train Epoch: [18][16/17]	Loss 0.4740 (0.6514)	LossFusion 0.4740 (0.6514)	
R@10:  54.79841232299805     R@50:  81.09028935432434
Mean Now:  67.9443508386612  Best Mean Before:  68.51221 --------------------
Train Epoch: [19][0/17]	Loss 0.8250 (0.8250)	LossFusion 0.8250 (0.8250)	
Train Epoch: [19][16/17]	Loss 0.4695 (0.6258)	LossFusion 0.4695 (0.6258)	
R@10:  54.45769429206848     R@50:  81.26064538955688
Mean Now:  67.85916984081268  Best Mean Before:  68.51221 --------------------
Train Epoch: [20][0/17]	Loss 0.7421 (0.7421)	LossFusion 0.7421 (0.7421)	
Train Epoch: [20][16/17]	Loss 0.4424 (0.5863)	LossFusion 0.4424 (0.5863)	
R@10:  54.85519766807556     R@50:  81.20386004447937
Mean Now:  68.02952885627747  Best Mean Before:  68.51221 --------------------
Train Epoch: [21][0/17]	Loss 0.7169 (0.7169)	LossFusion 0.7169 (0.7169)	
Train Epoch: [21][16/17]	Loss 0.3875 (0.5759)	LossFusion 0.3875 (0.5759)	
R@10:  55.139124393463135     R@50:  81.09028935432434
Mean Now:  68.11470687389374  Best Mean Before:  68.51221 --------------------
Train Epoch: [22][0/17]	Loss 0.6864 (0.6864)	LossFusion 0.6864 (0.6864)	
Train Epoch: [22][16/17]	Loss 0.4191 (0.5326)	LossFusion 0.4191 (0.5326)	
R@10:  54.68483567237854     R@50:  80.69278597831726
Mean Now:  67.6888108253479  Best Mean Before:  68.51221 --------------------
Train Epoch: [23][0/17]	Loss 0.6414 (0.6414)	LossFusion 0.6414 (0.6414)	
Train Epoch: [23][16/17]	Loss 0.3342 (0.5067)	LossFusion 0.3342 (0.5067)	
R@10:  55.36627173423767     R@50:  81.14707469940186
Mean Now:  68.25667321681976  Best Mean Before:  68.51221 --------------------
Train Epoch: [24][0/17]	Loss 0.6148 (0.6148)	LossFusion 0.6148 (0.6148)	
Train Epoch: [24][16/17]	Loss 0.3519 (0.4881)	LossFusion 0.3519 (0.4881)	
R@10:  54.85519766807556     R@50:  81.3174307346344
Mean Now:  68.08631420135498  Best Mean Before:  68.51221 --------------------
Train Epoch: [25][0/17]	Loss 0.5362 (0.5362)	LossFusion 0.5362 (0.5362)	
Train Epoch: [25][16/17]	Loss 0.3900 (0.4777)	LossFusion 0.3900 (0.4777)	
R@10:  54.79841232299805     R@50:  81.3174307346344
Mean Now:  68.05792152881622  Best Mean Before:  68.51221 --------------------
Train Epoch: [26][0/17]	Loss 0.5377 (0.5377)	LossFusion 0.5377 (0.5377)	
