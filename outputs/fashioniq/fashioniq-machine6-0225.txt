/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
=> world size: 8
=> rank: 6
=> dist_url: tcp://127.0.0.1:29500
=> local_rank: 6=> world size:
 8
=> rank: 7
=> dist_url: tcp://127.0.0.1:29500
=> local_rank: 7
=> world size: 8
=> rank: 0=> world size:
 => dist_url:=> world size:8 => world size: 
tcp://127.0.0.1:29500 8=> rank:
8
 => local_rank:
=> rank:1 => rank: 
0 2=> dist_url:
4
 
=> dist_url:tcp://127.0.0.1:29500=> dist_url: 
 tcp://127.0.0.1:29500=> local_rank:tcp://127.0.0.1:29500
 
=> local_rank:1=> local_rank: 
 24

=> world size: 8
=> rank: 3
=> dist_url: tcp://127.0.0.1:29500
=> local_rank: 3
=> world size: 8
=> rank: 5
=> dist_url: tcp://127.0.0.1:29500
=> local_rank: 5
CLIP model loaded successfully
Prompt learner loader successfully
CLIP model loaded successfully
CLIP model loaded successfully
CLIP model loaded successfully
Prompt learner loader successfully
CLIP model loaded successfully
Prompt learner loader successfully
Prompt learner loader successfully
Prompt learner loader successfully
CLIP model loaded successfully
Prompt learner loader successfully
CLIP model loaded successfully
Prompt learner loader successfully
CLIP model loaded successfully
Prompt learner loader successfully
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initializedFashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized

FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized
FashionIQ train - ['dress', 'toptee', 'shirt'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in relative mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['dress'] dataset in classic mode initialized
FashionIQ val - ['toptee'] dataset in relative mode initialized
FashionIQ val - ['toptee'] dataset in relative mode initialized
FashionIQ val - ['toptee'] dataset in relative mode initializedFashionIQ val - ['toptee'] dataset in relative mode initialized

FashionIQ val - ['toptee'] dataset in classic mode initialized
FashionIQ val - ['toptee'] dataset in classic mode initialized
FashionIQ val - ['toptee'] dataset in classic mode initializedFashionIQ val - ['toptee'] dataset in classic mode initialized

FashionIQ val - ['toptee'] dataset in relative mode initialized
FashionIQ val - ['toptee'] dataset in relative mode initialized
FashionIQ val - ['toptee'] dataset in relative mode initialized
FashionIQ val - ['toptee'] dataset in classic mode initialized
FashionIQ val - ['toptee'] dataset in classic mode initialized
FashionIQ val - ['toptee'] dataset in classic mode initialized
FashionIQ val - ['toptee'] dataset in relative mode initialized
FashionIQ val - ['toptee'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in relative mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
FashionIQ val - ['shirt'] dataset in classic mode initialized
Begin to train
Begin to train
Begin to train
Begin to train
Begin to train
Begin to train
Begin to train
Begin to train
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
/mnt/vision_retrieval/chenyanzhe/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Train Epoch: [0][0/17]	Loss 11.1607 (11.1607)	LossFusion 11.1607 (11.1607)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
Train Epoch: [0][16/17]	Loss 4.6444 (7.5878)	LossFusion 4.6444 (7.5878)	
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
R@10:  36.59879962603251     R@50:  60.628485679626465
Best Mean Now:  48.61364 ******************************
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][0/17]	Loss 5.2398 (5.2398)	LossFusion 5.2398 (5.2398)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
Train Epoch: [1][16/17]	Loss 3.6410 (4.3432)	LossFusion 3.6410 (4.3432)	
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
R@10:  40.848684310913086     R@50:  63.93162806828817
Best Mean Now:  52.39016 ******************************
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][0/17]	Loss 4.6566 (4.6566)	LossFusion 4.6566 (4.6566)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
Train Epoch: [2][16/17]	Loss 3.2791 (3.8388)	LossFusion 3.2791 (3.8388)	
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
R@10:  42.12342003981272     R@50:  64.87193306287129
Best Mean Now:  53.49768 ******************************
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][0/17]	Loss 4.1696 (4.1696)	LossFusion 4.1696 (4.1696)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
Train Epoch: [3][16/17]	Loss 3.1361 (3.6233)	LossFusion 3.1361 (3.6233)	
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
R@10:  42.43832230567932     R@50:  65.63870708147685
Best Mean Now:  54.03851 ******************************
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][0/17]	Loss 3.9974 (3.9974)	LossFusion 3.9974 (3.9974)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
Train Epoch: [4][16/17]	Loss 3.0893 (3.4679)	LossFusion 3.0893 (3.4679)	
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
R@10:  42.41512815157572     R@50:  65.26198387145996
Mean Now:  53.83855601151784  Best Mean Before:  54.03851 --------------------
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][0/17]	Loss 4.0033 (4.0033)	LossFusion 4.0033 (4.0033)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
Train Epoch: [5][16/17]	Loss 2.9717 (3.3953)	LossFusion 2.9717 (3.3953)	
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
R@10:  43.126882115999855     R@50:  65.66760540008545
Best Mean Now:  54.39724 ******************************
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][0/17]	Loss 3.9014 (3.9014)	LossFusion 3.9014 (3.9014)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
Train Epoch: [6][16/17]	Loss 2.9116 (3.3298)	LossFusion 2.9116 (3.3298)	
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
R@10:  43.35169494152069     R@50:  65.91516931851704
Best Mean Now:  54.63343 ******************************
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][0/17]	Loss 3.7153 (3.7153)	LossFusion 3.7153 (3.7153)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
Train Epoch: [7][16/17]	Loss 2.9099 (3.2485)	LossFusion 2.9099 (3.2485)	
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
R@10:  43.07618935902914     R@50:  65.83219567934673
Mean Now:  54.45419251918793  Best Mean Before:  54.63343 --------------------
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][0/17]	Loss 3.7857 (3.7857)	LossFusion 3.7857 (3.7857)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
Train Epoch: [8][16/17]	Loss 2.8623 (3.2084)	LossFusion 2.8623 (3.2084)	
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
R@10:  43.404615918795265     R@50:  66.03277921676636
Best Mean Now:  54.7187 ******************************
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][0/17]	Loss 3.7692 (3.7692)	LossFusion 3.7692 (3.7692)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
Train Epoch: [9][16/17]	Loss 2.7991 (3.1771)	LossFusion 2.7991 (3.1771)	
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
R@10:  43.590048948923744     R@50:  66.20912949244182
Best Mean Now:  54.89959 ******************************
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][0/17]	Loss 3.6689 (3.6689)	LossFusion 3.6689 (3.6689)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
Train Epoch: [10][16/17]	Loss 2.6930 (3.1032)	LossFusion 2.6930 (3.1032)	
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
R@10:  43.89219979445139     R@50:  66.40265981356303
Best Mean Now:  55.14743 ******************************
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][0/17]	Loss 3.6042 (3.6042)	LossFusion 3.6042 (3.6042)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
Train Epoch: [11][16/17]	Loss 2.7488 (3.1207)	LossFusion 2.7488 (3.1207)	
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
R@10:  43.42610935370127     R@50:  66.56616727511089
Mean Now:  54.99613831440608  Best Mean Before:  55.14743 --------------------
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][0/17]	Loss 3.5457 (3.5457)	LossFusion 3.5457 (3.5457)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
Train Epoch: [12][16/17]	Loss 2.6882 (3.0501)	LossFusion 2.6882 (3.0501)	
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
R@10:  43.47287118434906     R@50:  66.46568775177002
Mean Now:  54.96927946805954  Best Mean Before:  55.14743 --------------------
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][0/17]	Loss 3.5768 (3.5768)	LossFusion 3.5768 (3.5768)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
Train Epoch: [13][16/17]	Loss 2.5781 (3.0166)	LossFusion 2.5781 (3.0166)	
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
R@10:  43.68754227956136     R@50:  66.68190161387126
Best Mean Now:  55.18472 ******************************
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][0/17]	Loss 3.5765 (3.5765)	LossFusion 3.5765 (3.5765)	
Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	
Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	
Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	

Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	
Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	
Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	
Train Epoch: [14][16/17]	Loss 2.5976 (2.9965)	LossFusion 2.5976 (2.9965)	
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
R@10:  43.85906954606374     R@50:  66.54857993125916
Best Mean Now:  55.20382 ******************************
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][0/17]	Loss 3.4108 (3.4108)	LossFusion 3.4108 (3.4108)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
Train Epoch: [15][16/17]	Loss 2.5366 (2.9480)	LossFusion 2.5366 (2.9480)	
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
R@10:  44.0750390291214     R@50:  66.49798154830933
Best Mean Now:  55.28651 ******************************
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][0/17]	Loss 3.4547 (3.4547)	LossFusion 3.4547 (3.4547)	
Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	
Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	

Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	
Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	
Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	
Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	
Train Epoch: [16][16/17]	Loss 2.4654 (2.9212)	LossFusion 2.4654 (2.9212)	
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
R@10:  44.37600870927175     R@50:  66.63340131441753
Best Mean Now:  55.50471 ******************************
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][0/17]	Loss 3.3979 (3.3979)	LossFusion 3.3979 (3.3979)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
Train Epoch: [17][16/17]	Loss 2.5068 (2.8818)	LossFusion 2.5068 (2.8818)	
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
R@10:  44.54222917556763     R@50:  66.76538586616516
Best Mean Now:  55.65381 ******************************
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][0/17]	Loss 3.4098 (3.4098)	LossFusion 3.4098 (3.4098)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
Train Epoch: [18][16/17]	Loss 2.4963 (2.8716)	LossFusion 2.4963 (2.8716)	
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
R@10:  44.073477387428284     R@50:  66.46721959114075
Mean Now:  55.270348489284515  Best Mean Before:  55.65381 --------------------
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][0/17]	Loss 3.3630 (3.3630)	LossFusion 3.3630 (3.3630)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
Train Epoch: [19][16/17]	Loss 2.5476 (2.8703)	LossFusion 2.5476 (2.8703)	
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
R@10:  44.14065778255463     R@50:  66.99816981951396
Mean Now:  55.569413801034294  Best Mean Before:  55.65381 --------------------
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][0/17]	Loss 3.3157 (3.3157)	LossFusion 3.3157 (3.3157)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
Train Epoch: [20][16/17]	Loss 2.4635 (2.8228)	LossFusion 2.4635 (2.8228)	
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
R@10:  44.18955445289612     R@50:  66.98105533917744
Mean Now:  55.58530489603678  Best Mean Before:  55.65381 --------------------
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][0/17]	Loss 3.3826 (3.3826)	LossFusion 3.3826 (3.3826)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
Train Epoch: [21][16/17]	Loss 2.4916 (2.8190)	LossFusion 2.4916 (2.8190)	
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
R@10:  44.27231748898824     R@50:  67.16554363568623
Best Mean Now:  55.71893 ******************************
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][0/17]	Loss 3.3316 (3.3316)	LossFusion 3.3316 (3.3316)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
Train Epoch: [22][16/17]	Loss 2.3611 (2.7838)	LossFusion 2.3611 (2.7838)	
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
R@10:  44.371513525644936     R@50:  66.96611444155376
Mean Now:  55.66881398359935  Best Mean Before:  55.71893 --------------------
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	
Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	Train Epoch: [23][0/17]	Loss 3.3066 (3.3066)	LossFusion 3.3066 (3.3066)	

Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
Train Epoch: [23][16/17]	Loss 2.3469 (2.7477)	LossFusion 2.3469 (2.7477)	
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
R@10:  44.508665800094604     R@50:  67.19645857810974
Best Mean Now:  55.85256 ******************************
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][0/17]	Loss 3.3309 (3.3309)	LossFusion 3.3309 (3.3309)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
Train Epoch: [24][16/17]	Loss 2.4076 (2.7560)	LossFusion 2.4076 (2.7560)	
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
R@10:  44.456674655278526     R@50:  67.19427108764648
Mean Now:  55.82547287146251  Best Mean Before:  55.85256 --------------------
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][0/17]	Loss 3.2165 (3.2165)	LossFusion 3.2165 (3.2165)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
Train Epoch: [25][16/17]	Loss 2.3763 (2.7415)	LossFusion 2.3763 (2.7415)	
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
R@10:  44.61265901724497     R@50:  67.12991992632548
Best Mean Now:  55.87129 ******************************
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][0/17]	Loss 3.2624 (3.2624)	LossFusion 3.2624 (3.2624)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
Train Epoch: [26][16/17]	Loss 2.3078 (2.7294)	LossFusion 2.3078 (2.7294)	
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
R@10:  44.824512799580894     R@50:  67.23204056421916
Best Mean Now:  56.02828 ******************************
Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	
Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	

Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	
Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	
Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	
Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	
Train Epoch: [27][0/17]	Loss 3.2287 (3.2287)	LossFusion 3.2287 (3.2287)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
Train Epoch: [27][16/17]	Loss 2.3347 (2.6970)	LossFusion 2.3347 (2.6970)	
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
R@10:  44.312054912249245     R@50:  67.05233653386433
Mean Now:  55.68219572305679  Best Mean Before:  56.02828 --------------------
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][0/17]	Loss 3.2320 (3.2320)	LossFusion 3.2320 (3.2320)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
Train Epoch: [28][16/17]	Loss 2.3353 (2.6845)	LossFusion 2.3353 (2.6845)	
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
R@10:  44.50492958227793     R@50:  66.9978658358256
Mean Now:  55.75139770905177  Best Mean Before:  56.02828 --------------------
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][0/17]	Loss 3.2188 (3.2188)	LossFusion 3.2188 (3.2188)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
Train Epoch: [29][16/17]	Loss 2.3004 (2.6568)	LossFusion 2.3004 (2.6568)	
R@10:  44.773084918657936     R@50:  67.063041528066
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
R@10:  44.773084918657936     R@50:  67.063041528066
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
R@10:  44.773084918657936     R@50:  67.063041528066
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
R@10:  44.773084918657936     R@50:  67.063041528066
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
R@10:  44.773084918657936     R@50:  67.063041528066
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
R@10:  44.773084918657936     R@50:  67.063041528066R@10: 
 Mean Now: 44.773084918657936  55.91806322336197    R@50:    Best Mean Before: 67.063041528066 
56.02828 --------------------
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
R@10:  44.773084918657936     R@50:  67.063041528066
Mean Now:  55.91806322336197  Best Mean Before:  56.02828 --------------------
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][0/17]	Loss 3.1815 (3.1815)	LossFusion 3.1815 (3.1815)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
Train Epoch: [30][16/17]	Loss 2.2893 (2.6620)	LossFusion 2.2893 (2.6620)	
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
R@10:  44.80777780214945     R@50:  67.1815574169159
Mean Now:  55.99466760953267  Best Mean Before:  56.02828 --------------------
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][0/17]	Loss 3.2007 (3.2007)	LossFusion 3.2007 (3.2007)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
Train Epoch: [31][16/17]	Loss 2.2719 (2.6395)	LossFusion 2.2719 (2.6395)	
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
R@10:  44.593959053357445     R@50:  67.12984442710876
Mean Now:  55.86190174023311  Best Mean Before:  56.02828 --------------------
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][0/17]	Loss 3.2528 (3.2528)	LossFusion 3.2528 (3.2528)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
Train Epoch: [32][16/17]	Loss 2.2933 (2.6473)	LossFusion 2.2933 (2.6473)	
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
R@10:  44.59335505962372     R@50:  67.31149951616923
Mean Now:  55.95242728789648  Best Mean Before:  56.02828 --------------------
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][0/17]	Loss 3.1952 (3.1952)	LossFusion 3.1952 (3.1952)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
Train Epoch: [33][16/17]	Loss 2.1560 (2.6191)	LossFusion 2.1560 (2.6191)	
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
R@10:  44.60978806018829     R@50:  67.39885012308757
Mean Now:  56.00431909163793  Best Mean Before:  56.02828 --------------------
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][0/17]	Loss 3.2403 (3.2403)	LossFusion 3.2403 (3.2403)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
Train Epoch: [34][16/17]	Loss 2.2094 (2.6195)	LossFusion 2.2094 (2.6195)	
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
R@10:  44.36099032560984     R@50:  67.11320877075195
Mean Now:  55.73709954818089  Best Mean Before:  56.02828 --------------------
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][0/17]	Loss 3.1851 (3.1851)	LossFusion 3.1851 (3.1851)	
Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	
Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	

Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	
Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	
Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	
Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	
Train Epoch: [35][16/17]	Loss 2.2506 (2.5897)	LossFusion 2.2506 (2.5897)	
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
R@10:  44.99041736125946     R@50:  67.07835992177327
Best Mean Now:  56.03439 ******************************
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][0/17]	Loss 3.0715 (3.0715)	LossFusion 3.0715 (3.0715)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
Train Epoch: [36][16/17]	Loss 2.1824 (2.5835)	LossFusion 2.1824 (2.5835)	
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
R@10:  44.95783646901449     R@50:  66.98443690935771
Mean Now:  55.971136689186096  Best Mean Before:  56.03439 --------------------
Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	
Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	
Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	

Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	
Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	
Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	
Train Epoch: [37][0/17]	Loss 3.0958 (3.0958)	LossFusion 3.0958 (3.0958)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
Train Epoch: [37][16/17]	Loss 2.2325 (2.5534)	LossFusion 2.2325 (2.5534)	
R@10:  44.57391897837321     R@50:  67.00101296106975
Mean Now:  55.78746596972148  Best Mean Before:  56.03439 --------------------
R@10:  44.57391897837321     R@50:  67.00101296106975
Mean Now:  55.78746596972148  Best Mean Before:  56.03439 --------------------
R@10:  44.57391897837321     R@50:  67.00101296106975
Mean Now:  55.78746596972148  Best Mean Before:  56.03439 --------------------
R@10:  44.57391897837321     R@50:  67.00101296106975
Mean Now:  55.78746596972148  Best Mean Before:  56.03439 --------------------
R@10:  44.57391897837321     R@50:  67.00101296106975
Mean Now:  55.78746596972148  Best Mean Before:  56.03439 --------------------
R@10: R@10:   44.57391897837321     R@50: 44.57391897837321  67.00101296106975    R@50: 
 67.00101296106975Mean Now: 
 55.78746596972148  Best Mean Before: Mean Now:   56.0343955.78746596972148  -------------------- Best Mean Before: 
 56.03439 --------------------
R@10:  44.57391897837321     R@50:  67.00101296106975
Mean Now:  55.78746596972148  Best Mean Before:  56.03439 --------------------
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][0/17]	Loss 3.0663 (3.0663)	LossFusion 3.0663 (3.0663)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
Train Epoch: [38][16/17]	Loss 2.1858 (2.5511)	LossFusion 2.1858 (2.5511)	
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
R@10:  44.78875994682312     R@50:  67.42997765541077
Best Mean Now:  56.10937 ******************************
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][0/17]	Loss 3.1251 (3.1251)	LossFusion 3.1251 (3.1251)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
Train Epoch: [39][16/17]	Loss 2.2091 (2.5442)	LossFusion 2.2091 (2.5442)	
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
R@10:  45.00797986984253     R@50:  67.49538381894429
Best Mean Now:  56.25168 ******************************
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][0/17]	Loss 3.0753 (3.0753)	LossFusion 3.0753 (3.0753)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
Train Epoch: [40][16/17]	Loss 2.0347 (2.5337)	LossFusion 2.0347 (2.5337)	
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
R@10:  44.52831149101257     R@50:  66.99658036231995
Mean Now:  55.76244592666626  Best Mean Before:  56.25168 --------------------
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][0/17]	Loss 3.0901 (3.0901)	LossFusion 3.0901 (3.0901)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
Train Epoch: [41][16/17]	Loss 2.0845 (2.5411)	LossFusion 2.0845 (2.5411)	
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
R@10:  44.57322359085083     R@50:  67.39643613497417
Mean Now:  55.9848298629125  Best Mean Before:  56.25168 --------------------
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][0/17]	Loss 3.0952 (3.0952)	LossFusion 3.0952 (3.0952)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
Train Epoch: [42][16/17]	Loss 2.0908 (2.5358)	LossFusion 2.0908 (2.5358)	
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
R@10:  44.89318529764811     R@50:  67.00011094411214
Mean Now:  55.94664812088013  Best Mean Before:  56.25168 --------------------
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][0/17]	Loss 3.0722 (3.0722)	LossFusion 3.0722 (3.0722)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
Train Epoch: [43][16/17]	Loss 2.1371 (2.5242)	LossFusion 2.1371 (2.5242)	
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
R@10:  44.50626770655314     R@50:  67.27948387463887
Mean Now:  55.89287579059601  Best Mean Before:  56.25168 --------------------
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][0/17]	Loss 3.0894 (3.0894)	LossFusion 3.0894 (3.0894)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
Train Epoch: [44][16/17]	Loss 2.1586 (2.4933)	LossFusion 2.1586 (2.4933)	
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
R@10:  45.15941639741262     R@50:  67.48173038164775
Best Mean Now:  56.32057 ******************************
Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	
Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	
Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	

Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	
Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	
Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	
Train Epoch: [45][0/17]	Loss 2.9713 (2.9713)	LossFusion 2.9713 (2.9713)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
Train Epoch: [45][16/17]	Loss 2.0745 (2.4742)	LossFusion 2.0745 (2.4742)	
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
R@10:  44.71319317817688     R@50:  67.10337003072102
Mean Now:  55.90828160444895  Best Mean Before:  56.32057 --------------------
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][0/17]	Loss 3.1024 (3.1024)	LossFusion 3.1024 (3.1024)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
Train Epoch: [46][16/17]	Loss 1.9806 (2.4710)	LossFusion 1.9806 (2.4710)	
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
R@10:  45.110045870145164     R@50:  67.29860504468282
Mean Now:  56.20432545741399  Best Mean Before:  56.32057 --------------------
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][0/17]	Loss 3.0227 (3.0227)	LossFusion 3.0227 (3.0227)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
Train Epoch: [47][16/17]	Loss 2.1036 (2.4569)	LossFusion 2.1036 (2.4569)	
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
R@10:  44.611466924349465     R@50:  67.26150314013164
Mean Now:  55.936485032240554  Best Mean Before:  56.32057 --------------------
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][0/17]	Loss 3.0274 (3.0274)	LossFusion 3.0274 (3.0274)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
Train Epoch: [48][16/17]	Loss 2.0778 (2.4587)	LossFusion 2.0778 (2.4587)	
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
R@10:  44.945465524991356     R@50:  67.34434366226196
Mean Now:  56.14490459362666  Best Mean Before:  56.32057 --------------------
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][0/17]	Loss 2.9328 (2.9328)	LossFusion 2.9328 (2.9328)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
Train Epoch: [49][16/17]	Loss 1.9760 (2.4339)	LossFusion 1.9760 (2.4339)	
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
R@10:  45.031097531318665     R@50:  67.34570463498433
Mean Now:  56.1884010831515  Best Mean Before:  56.32057 --------------------
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][0/17]	Loss 2.9197 (2.9197)	LossFusion 2.9197 (2.9197)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
Train Epoch: [50][16/17]	Loss 2.0423 (2.4269)	LossFusion 2.0423 (2.4269)	
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
R@10:  45.24384240309397     R@50:  67.33097434043884
Mean Now:  56.2874083717664  Best Mean Before:  56.32057 --------------------
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][0/17]	Loss 2.9626 (2.9626)	LossFusion 2.9626 (2.9626)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
Train Epoch: [51][16/17]	Loss 1.9767 (2.4085)	LossFusion 1.9767 (2.4085)	
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
R@10:  45.27809917926788     R@50:  67.16756224632263
Mean Now:  56.22283071279526  Best Mean Before:  56.32057 --------------------
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][0/17]	Loss 2.9561 (2.9561)	LossFusion 2.9561 (2.9561)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
Train Epoch: [52][16/17]	Loss 1.9250 (2.3951)	LossFusion 1.9250 (2.3951)	
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
R@10:  44.97324923674265     R@50:  67.56003499031067
Mean Now:  56.26664211352666  Best Mean Before:  56.32057 --------------------
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][0/17]	Loss 3.0162 (3.0162)	LossFusion 3.0162 (3.0162)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
Train Epoch: [53][16/17]	Loss 1.9660 (2.3940)	LossFusion 1.9660 (2.3940)	
R@10:  45.02831995487213     R@50:  67.47851967811584
Mean Now:  56.25341981649399  Best Mean Before:  56.32057 --------------------
R@10:  45.02831995487213     R@50:  67.47851967811584
Mean Now:  56.25341981649399  Best Mean Before:  56.32057 --------------------
R@10:  45.02831995487213     R@50:  67.47851967811584
Mean Now:  56.25341981649399  Best Mean Before:  56.32057 --------------------
R@10: R@10:   45.0283199548721345.02831995487213      R@50:     R@50:   67.4785196781158467.47851967811584

Mean Now:  Mean Now: 56.25341981649399  56.25341981649399 Best Mean Before:    Best Mean Before: 56.32057  --------------------56.32057
 --------------------
R@10:  45.02831995487213     R@50:  67.47851967811584
Mean Now:  56.25341981649399  Best Mean Before:  56.32057 --------------------
R@10:  45.02831995487213     R@50:  67.47851967811584
Mean Now:  56.25341981649399  Best Mean Before:  56.32057 --------------------
R@10:  45.02831995487213     R@50:  67.47851967811584
Mean Now:  56.25341981649399  Best Mean Before:  56.32057 --------------------
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][0/17]	Loss 2.9797 (2.9797)	LossFusion 2.9797 (2.9797)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
Train Epoch: [54][16/17]	Loss 1.9988 (2.4076)	LossFusion 1.9988 (2.4076)	
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
R@10:  45.09750405947367     R@50:  66.99748833974202
Mean Now:  56.04749619960785  Best Mean Before:  56.32057 --------------------
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][0/17]	Loss 3.0111 (3.0111)	LossFusion 3.0111 (3.0111)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
Train Epoch: [55][16/17]	Loss 2.0419 (2.4017)	LossFusion 2.0419 (2.4017)	
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
R@10:  44.976723194122314     R@50:  67.52720475196838
Mean Now:  56.25196397304535  Best Mean Before:  56.32057 --------------------
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][0/17]	Loss 3.0214 (3.0214)	LossFusion 3.0214 (3.0214)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
Train Epoch: [56][16/17]	Loss 1.9694 (2.3893)	LossFusion 1.9694 (2.3893)	
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
R@10:  45.17820874849955     R@50:  67.41433938344319
Mean Now:  56.296274065971375  Best Mean Before:  56.32057 --------------------
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][0/17]	Loss 2.9862 (2.9862)	LossFusion 2.9862 (2.9862)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
Train Epoch: [57][16/17]	Loss 1.9105 (2.3732)	LossFusion 1.9105 (2.3732)	
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
R@10:  44.929241140683494     R@50:  67.3322598139445
Mean Now:  56.130750477313995  Best Mean Before:  56.32057 --------------------
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][0/17]	Loss 2.9777 (2.9777)	LossFusion 2.9777 (2.9777)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
Train Epoch: [58][16/17]	Loss 1.9766 (2.3759)	LossFusion 1.9766 (2.3759)	
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
R@10:  45.11184096336365     R@50:  67.33080546061198
Mean Now:  56.221323211987816  Best Mean Before:  56.32057 --------------------
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][0/17]	Loss 2.9598 (2.9598)	LossFusion 2.9598 (2.9598)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
Train Epoch: [59][16/17]	Loss 1.9120 (2.3511)	LossFusion 1.9120 (2.3511)	
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
R@10:  45.07776697476705     R@50:  67.36394961675008
Mean Now:  56.22085829575856  Best Mean Before:  56.32057 --------------------
Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	

Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	
Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	
Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	
Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	
Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	
Train Epoch: [60][0/17]	Loss 2.9509 (2.9509)	LossFusion 2.9509 (2.9509)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
Train Epoch: [60][16/17]	Loss 1.9515 (2.3606)	LossFusion 1.9515 (2.3606)	
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
R@10:  44.92910901705424     R@50:  67.45963295300801
Mean Now:  56.19437098503113  Best Mean Before:  56.32057 --------------------
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][0/17]	Loss 2.8375 (2.8375)	LossFusion 2.8375 (2.8375)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
Train Epoch: [61][16/17]	Loss 1.9834 (2.3466)	LossFusion 1.9834 (2.3466)	
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
R@10:  45.01160879929861     R@50:  67.41147041320801
Mean Now:  56.21153960625331  Best Mean Before:  56.32057 --------------------
Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	
Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	
Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	

Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	
Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	
Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	
Train Epoch: [62][0/17]	Loss 2.8549 (2.8549)	LossFusion 2.8549 (2.8549)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
Train Epoch: [62][16/17]	Loss 1.9675 (2.3467)	LossFusion 1.9675 (2.3467)	
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
R@10:  45.02843717734019     R@50:  67.23104317982991
Mean Now:  56.12974017858505  Best Mean Before:  56.32057 --------------------
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][0/17]	Loss 2.8749 (2.8749)	LossFusion 2.8749 (2.8749)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
Train Epoch: [63][16/17]	Loss 1.9061 (2.3345)	LossFusion 1.9061 (2.3345)	
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
R@10:  45.01268366972605     R@50:  67.3471987247467
Mean Now:  56.179941197236374  Best Mean Before:  56.32057 --------------------
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][0/17]	Loss 2.9314 (2.9314)	LossFusion 2.9314 (2.9314)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
Train Epoch: [64][16/17]	Loss 1.9143 (2.3320)	LossFusion 1.9143 (2.3320)	
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
R@10:  45.179060101509094     R@50:  67.44580666224162
Mean Now:  56.31243338187536  Best Mean Before:  56.32057 --------------------
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][0/17]	Loss 2.8837 (2.8837)	LossFusion 2.8837 (2.8837)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
Train Epoch: [65][16/17]	Loss 1.8622 (2.3382)	LossFusion 1.8622 (2.3382)	
R@10:  44.95963156223297     R@50:  67.36463109652202
Mean Now:  56.162131329377495  Best Mean Before:  56.32057 --------------------
R@10: R@10:   44.95963156223297 44.95963156223297    R@50:       R@50: 67.36463109652202 
67.36463109652202
Mean Now:  56.162131329377495Mean Now:    Best Mean Before: 56.162131329377495  56.32057 Best Mean Before:   --------------------56.32057
 --------------------
R@10:  44.95963156223297     R@50:  67.36463109652202
Mean Now:  56.162131329377495  Best Mean Before:  56.32057 --------------------
R@10:  44.95963156223297     R@50:  67.36463109652202
Mean Now:  56.162131329377495  Best Mean Before:  56.32057 --------------------
R@10:  44.95963156223297     R@50:  67.36463109652202
Mean Now:  56.162131329377495  Best Mean Before:  56.32057 --------------------
R@10:  44.95963156223297     R@50:  67.36463109652202
Mean Now:  56.162131329377495  Best Mean Before:  56.32057 --------------------
R@10:  44.95963156223297     R@50:  67.36463109652202
Mean Now:  56.162131329377495  Best Mean Before:  56.32057 --------------------
Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	
Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	
Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	

Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	
Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	
Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	
Train Epoch: [66][0/17]	Loss 2.8786 (2.8786)	LossFusion 2.8786 (2.8786)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
Train Epoch: [66][16/17]	Loss 1.9280 (2.3198)	LossFusion 1.9280 (2.3198)	
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
R@10:  45.015632112820946     R@50:  67.44438807169597
Mean Now:  56.23001009225845  Best Mean Before:  56.32057 --------------------
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][0/17]	Loss 2.8824 (2.8824)	LossFusion 2.8824 (2.8824)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
Train Epoch: [67][16/17]	Loss 1.9114 (2.3291)	LossFusion 1.9114 (2.3291)	
R@10:  44.96429761250814     R@50:  67.42880741755168
Mean Now:  56.19655251502991  Best Mean Before:  56.32057 --------------------
R@10:  44.96429761250814     R@50:  67.42880741755168
Mean Now:  56.19655251502991  Best Mean Before:  56.32057 --------------------
R@10:  44.96429761250814     R@50:  67.42880741755168
Mean Now:  56.19655251502991  Best Mean Before:  56.32057 --------------------
R@10:  R@10: 44.96429761250814      R@50:  67.4288074175516844.96429761250814
     R@50:  Mean Now: 67.42880741755168 
56.19655251502991  Best Mean Before:  Mean Now: 56.32057  56.19655251502991-------------------- 
 Best Mean Before:  56.32057 --------------------
R@10:  44.96429761250814     R@50:  67.42880741755168
Mean Now:  56.19655251502991  Best Mean Before:  56.32057 --------------------
R@10:  44.96429761250814     R@50:  67.42880741755168
Mean Now:  56.19655251502991  Best Mean Before:  56.32057 --------------------
R@10:  44.96429761250814     R@50:  67.42880741755168
Mean Now:  56.19655251502991  Best Mean Before:  56.32057 --------------------
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][0/17]	Loss 2.8026 (2.8026)	LossFusion 2.8026 (2.8026)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
Train Epoch: [68][16/17]	Loss 1.9069 (2.3076)	LossFusion 1.9069 (2.3076)	
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
R@10:  44.9634850025177     R@50:  67.49589641888936
Mean Now:  56.22969071070353  Best Mean Before:  56.32057 --------------------
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
Train Epoch: [69][16/17]	Loss 1.9172 (2.2915)	LossFusion 1.9172 (2.2915)	
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
R@10:  44.96220052242279     R@50:  67.34540263811748
Mean Now:  56.153801580270134  Best Mean Before:  56.32057 --------------------
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][0/17]	Loss 2.8492 (2.8492)	LossFusion 2.8492 (2.8492)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
Train Epoch: [70][16/17]	Loss 1.8712 (2.2968)	LossFusion 1.8712 (2.2968)	
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
R@10:  45.11248469352722     R@50:  67.41386850674947
Mean Now:  56.263176600138344  Best Mean Before:  56.32057 --------------------
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][0/17]	Loss 2.8953 (2.8953)	LossFusion 2.8953 (2.8953)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
Train Epoch: [71][16/17]	Loss 1.8455 (2.2985)	LossFusion 1.8455 (2.2985)	
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
R@10:  45.144892732302345     R@50:  67.54492322603862
Best Mean Now:  56.34491 ******************************
Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	
Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	
Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	
Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	
Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	
Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	

Train Epoch: [72][0/17]	Loss 2.8460 (2.8460)	LossFusion 2.8460 (2.8460)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
Train Epoch: [72][16/17]	Loss 1.8517 (2.3013)	LossFusion 1.8517 (2.3013)	
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
R@10:  44.89656786123911     R@50:  67.56290396054585
Mean Now:  56.22973591089249  Best Mean Before:  56.34491 --------------------
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][0/17]	Loss 2.9494 (2.9494)	LossFusion 2.9494 (2.9494)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
Train Epoch: [73][16/17]	Loss 1.9263 (2.2978)	LossFusion 1.9263 (2.2978)	
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
R@10:  45.04590630531311     R@50:  67.51238107681274
Mean Now:  56.27914369106293  Best Mean Before:  56.34491 --------------------
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][0/17]	Loss 2.8144 (2.8144)	LossFusion 2.8144 (2.8144)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
Train Epoch: [74][16/17]	Loss 1.9006 (2.2929)	LossFusion 1.9006 (2.2929)	
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
R@10:  45.11625866095225     R@50:  67.41275389989217
Mean Now:  56.26450628042221  Best Mean Before:  56.34491 --------------------
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][0/17]	Loss 2.8720 (2.8720)	LossFusion 2.8720 (2.8720)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
Train Epoch: [75][16/17]	Loss 1.9083 (2.3074)	LossFusion 1.9083 (2.3074)	
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
R@10:  45.03255287806193     R@50:  67.39605863889058
Mean Now:  56.21430575847626  Best Mean Before:  56.34491 --------------------
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][0/17]	Loss 2.7869 (2.7869)	LossFusion 2.7869 (2.7869)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
Train Epoch: [76][16/17]	Loss 1.9703 (2.2753)	LossFusion 1.9703 (2.2753)	
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
R@10:  44.997783501942955     R@50:  67.59656071662903
Mean Now:  56.297172109285995  Best Mean Before:  56.34491 --------------------
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][0/17]	Loss 2.8398 (2.8398)	LossFusion 2.8398 (2.8398)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
Train Epoch: [77][16/17]	Loss 1.8623 (2.2836)	LossFusion 1.8623 (2.2836)	
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
R@10:  45.14738519986471     R@50:  67.47881968816121
Mean Now:  56.313102444012955  Best Mean Before:  56.34491 --------------------
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][0/17]	Loss 2.7894 (2.7894)	LossFusion 2.7894 (2.7894)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
Train Epoch: [78][16/17]	Loss 1.8728 (2.2756)	LossFusion 1.8728 (2.2756)	
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
R@10:  45.02809743086497     R@50:  67.59433150291443
Mean Now:  56.311214466889695  Best Mean Before:  56.34491 --------------------
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][0/17]	Loss 2.8103 (2.8103)	LossFusion 2.8103 (2.8103)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
Train Epoch: [79][16/17]	Loss 1.8646 (2.2810)	LossFusion 1.8646 (2.2810)	
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
R@10:  44.949149092038475     R@50:  67.5471544265747
Mean Now:  56.248151759306594  Best Mean Before:  56.34491 --------------------
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][0/17]	Loss 2.8626 (2.8626)	LossFusion 2.8626 (2.8626)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
Train Epoch: [80][16/17]	Loss 1.8686 (2.2869)	LossFusion 1.8686 (2.2869)	
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
R@10:  45.04809478918711     R@50:  67.49581893285115
Mean Now:  56.271956861019135  Best Mean Before:  56.34491 --------------------
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][0/17]	Loss 2.8643 (2.8643)	LossFusion 2.8643 (2.8643)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
Train Epoch: [81][16/17]	Loss 1.8359 (2.2772)	LossFusion 1.8359 (2.2772)	
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
R@10:  44.97980177402496     R@50:  67.64404376347859
Mean Now:  56.31192276875178  Best Mean Before:  56.34491 --------------------
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][0/17]	Loss 2.8454 (2.8454)	LossFusion 2.8454 (2.8454)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
Train Epoch: [82][16/17]	Loss 1.8685 (2.2616)	LossFusion 1.8685 (2.2616)	
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
R@10:  44.84651784102122     R@50:  67.51123070716858
Mean Now:  56.178874274094895  Best Mean Before:  56.34491 --------------------
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][0/17]	Loss 2.8319 (2.8319)	LossFusion 2.8319 (2.8319)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
Train Epoch: [83][16/17]	Loss 1.8344 (2.2563)	LossFusion 1.8344 (2.2563)	
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
R@10:  44.93163824081421     R@50:  67.44516491889954
Mean Now:  56.18840157985687  Best Mean Before:  56.34491 --------------------
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][0/17]	Loss 2.8347 (2.8347)	LossFusion 2.8347 (2.8347)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
Train Epoch: [84][16/17]	Loss 1.8981 (2.2985)	LossFusion 1.8981 (2.2985)	
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
R@10:  45.02998391787211     R@50:  67.54620869954427
Mean Now:  56.28809630870819  Best Mean Before:  56.34491 --------------------
Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	

Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	
Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	
Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	
Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	
Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	
Train Epoch: [85][0/17]	Loss 2.8194 (2.8194)	LossFusion 2.8194 (2.8194)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
Train Epoch: [85][16/17]	Loss 1.8531 (2.2906)	LossFusion 1.8531 (2.2906)	
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
R@10:  45.080676674842834     R@50:  67.4966315428416
Mean Now:  56.288654108842216  Best Mean Before:  56.34491 --------------------
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][0/17]	Loss 2.8265 (2.8265)	LossFusion 2.8265 (2.8265)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
Train Epoch: [86][16/17]	Loss 1.9163 (2.2953)	LossFusion 1.9163 (2.2953)	
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.56209333737691
Mean Now:  56.338298320770264  Best Mean Before:  56.34491 --------------------
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][0/17]	Loss 2.7447 (2.7447)	LossFusion 2.7447 (2.7447)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
Train Epoch: [87][16/17]	Loss 1.9072 (2.2682)	LossFusion 1.9072 (2.2682)	
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
R@10:  45.14768520991007     R@50:  67.49616066614787
Mean Now:  56.321922938028976  Best Mean Before:  56.34491 --------------------
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][0/17]	Loss 2.8952 (2.8952)	LossFusion 2.8952 (2.8952)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
Train Epoch: [88][16/17]	Loss 1.8454 (2.2919)	LossFusion 1.8454 (2.2919)	
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
R@10:  45.01345753669739     R@50:  67.51268704732259
Mean Now:  56.26307229200999  Best Mean Before:  56.34491 --------------------
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][0/17]	Loss 2.8320 (2.8320)	LossFusion 2.8320 (2.8320)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
Train Epoch: [89][16/17]	Loss 1.8336 (2.2607)	LossFusion 1.8336 (2.2607)	
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
R@10:  45.07986307144165     R@50:  67.49616066614787
Mean Now:  56.28801186879476  Best Mean Before:  56.34491 --------------------
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][0/17]	Loss 2.8330 (2.8330)	LossFusion 2.8330 (2.8330)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
Train Epoch: [90][16/17]	Loss 1.8975 (2.2721)	LossFusion 1.8975 (2.2721)	
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
R@10:  45.08128066857656     R@50:  67.51251618067424
Mean Now:  56.2968984246254  Best Mean Before:  56.34491 --------------------
Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	
Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	
Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	

Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	
Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	
Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	
Train Epoch: [91][0/17]	Loss 2.8111 (2.8111)	LossFusion 2.8111 (2.8111)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
Train Epoch: [91][16/17]	Loss 1.8199 (2.2752)	LossFusion 1.8199 (2.2752)	
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.49632954597473
Mean Now:  56.263864537080124  Best Mean Before:  56.34491 --------------------
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][0/17]	Loss 2.8282 (2.8282)	LossFusion 2.8282 (2.8282)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
Train Epoch: [92][16/17]	Loss 1.8311 (2.2762)	LossFusion 1.8311 (2.2762)	
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491R@10:  --------------------
 45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47933030128479
Mean Now:  56.25536491473515  Best Mean Before:  56.34491 --------------------
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][0/17]	Loss 2.8249 (2.8249)	LossFusion 2.8249 (2.8249)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
Train Epoch: [93][16/17]	Loss 1.8440 (2.2653)	LossFusion 1.8440 (2.2653)	
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.4786905447642
Mean Now:  56.27204328775406  Best Mean Before:  56.34491 --------------------
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][0/17]	Loss 2.8671 (2.8671)	LossFusion 2.8671 (2.8671)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
Train Epoch: [94][16/17]	Loss 1.9066 (2.2585)	LossFusion 1.9066 (2.2585)	
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.49504605929057
Mean Now:  56.28022104501724  Best Mean Before:  56.34491 --------------------
Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	
Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	
Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	

Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	
Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	
Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	
Train Epoch: [95][0/17]	Loss 2.8195 (2.8195)	LossFusion 2.8195 (2.8195)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
Train Epoch: [95][16/17]	Loss 1.8182 (2.2615)	LossFusion 1.8182 (2.2615)	
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][0/17]	Loss 2.8139 (2.8139)	LossFusion 2.8139 (2.8139)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
Train Epoch: [96][16/17]	Loss 1.8472 (2.2629)	LossFusion 1.8472 (2.2629)	
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][0/17]	Loss 2.8176 (2.8176)	LossFusion 2.8176 (2.8176)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
Train Epoch: [97][16/17]	Loss 1.8198 (2.2687)	LossFusion 1.8198 (2.2687)	
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][0/17]	Loss 2.8459 (2.8459)	LossFusion 2.8459 (2.8459)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
Train Epoch: [98][16/17]	Loss 1.8979 (2.2695)	LossFusion 1.8979 (2.2695)	
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 R@10: --------------------
 45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.47851967811584
Mean Now:  56.25495960315068  Best Mean Before:  56.34491 --------------------
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][0/17]	Loss 2.7841 (2.7841)	LossFusion 2.7841 (2.7841)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
Train Epoch: [99][16/17]	Loss 1.8553 (2.2638)	LossFusion 1.8553 (2.2638)	
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	
Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	

Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	
Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	
Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	
Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	
Train Epoch: [100][0/17]	Loss 2.8632 (2.8632)	LossFusion 2.8632 (2.8632)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
Train Epoch: [100][16/17]	Loss 1.8182 (2.2705)	LossFusion 1.8182 (2.2705)	
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][0/17]	Loss 2.8165 (2.8165)	LossFusion 2.8165 (2.8165)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
Train Epoch: [101][16/17]	Loss 1.8525 (2.2824)	LossFusion 1.8525 (2.2824)	
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][0/17]	Loss 2.9092 (2.9092)	LossFusion 2.9092 (2.9092)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
Train Epoch: [102][16/17]	Loss 1.8367 (2.2942)	LossFusion 1.8367 (2.2942)	
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
R@10:  45.048398772875466     R@50:  67.47851967811584
Mean Now:  56.26345922549565  Best Mean Before:  56.34491 --------------------
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][0/17]	Loss 2.7607 (2.7607)	LossFusion 2.7607 (2.7607)	
Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	
Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	
Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	

Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	
Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	
Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	
Train Epoch: [103][16/17]	Loss 1.9138 (2.2612)	LossFusion 1.9138 (2.2612)	
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
R@10:  45.06539603074392     R@50:  67.47851967811584
Mean Now:  56.271957854429886  Best Mean Before:  56.34491 --------------------
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][0/17]	Loss 2.8292 (2.8292)	LossFusion 2.8292 (2.8292)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
Train Epoch: [104][16/17]	Loss 1.8519 (2.2622)	LossFusion 1.8519 (2.2622)	
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031872391700745     R@50:  67.52792596817017
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][0/17]	Loss 2.8063 (2.8063)	LossFusion 2.8063 (2.8063)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
Train Epoch: [105][16/17]	Loss 1.7600 (2.2592)	LossFusion 1.7600 (2.2592)	
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795 R@10:  Best Mean Before:   56.34491 --------------------45.0309286514918
     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
R@10:  45.0309286514918     R@50:  67.5115704536438
Mean Now:  56.271249552567795  Best Mean Before:  56.34491 --------------------
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][0/17]	Loss 2.8207 (2.8207)	LossFusion 2.8207 (2.8207)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
Train Epoch: [106][16/17]	Loss 1.7141 (2.2404)	LossFusion 1.7141 (2.2404)	
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
R@10:  45.030454794565834     R@50:  67.52856771151225
Mean Now:  56.27951125303905  Best Mean Before:  56.34491 --------------------
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][0/17]	Loss 2.8621 (2.8621)	LossFusion 2.8621 (2.8621)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
Train Epoch: [107][16/17]	Loss 1.8536 (2.2806)	LossFusion 1.8536 (2.2806)	
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
R@10:  45.031399528185524     R@50:  67.52839883168538
Mean Now:  56.279899179935455  Best Mean Before:  56.34491 --------------------
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][0/17]	Loss 2.8445 (2.8445)	LossFusion 2.8445 (2.8445)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
Train Epoch: [108][16/17]	Loss 1.8323 (2.2626)	LossFusion 1.8323 (2.2626)	
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
R@10:  45.01504302024841     R@50:  67.57861971855164
Mean Now:  56.296831369400024  Best Mean Before:  56.34491 --------------------
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][0/17]	Loss 2.8804 (2.8804)	LossFusion 2.8804 (2.8804)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
Train Epoch: [109][16/17]	Loss 1.7616 (2.2577)	LossFusion 1.7616 (2.2577)	
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
R@10:  44.98186012109121     R@50:  67.56192247072856
Mean Now:  56.27189129590988  Best Mean Before:  56.34491 --------------------
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][0/17]	Loss 2.7961 (2.7961)	LossFusion 2.7961 (2.7961)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
Train Epoch: [110][16/17]	Loss 1.8748 (2.2787)	LossFusion 1.8748 (2.2787)	
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
R@10:  44.997912645339966     R@50:  67.49616066614787
Mean Now:  56.24703665574392  Best Mean Before:  56.34491 --------------------
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][0/17]	Loss 2.8302 (2.8302)	LossFusion 2.8302 (2.8302)	
Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	
Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	

Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	
Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	
Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	
Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	
Train Epoch: [111][16/17]	Loss 1.8649 (2.2913)	LossFusion 1.8649 (2.2913)	
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
R@10:  44.96533373991648     R@50:  67.52904256184895
Mean Now:  56.24718815088272  Best Mean Before:  56.34491 --------------------
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][0/17]	Loss 2.8711 (2.8711)	LossFusion 2.8711 (2.8711)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
Train Epoch: [112][16/17]	Loss 1.7939 (2.2713)	LossFusion 1.7939 (2.2713)	
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
R@10:  45.01298666000366     R@50:  67.49632954597473
Mean Now:  56.2546581029892  Best Mean Before:  56.34491 --------------------
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][0/17]	Loss 2.8986 (2.8986)	LossFusion 2.8986 (2.8986)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
Train Epoch: [113][16/17]	Loss 1.8514 (2.2844)	LossFusion 1.8514 (2.2844)	
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
R@10:  45.028358697891235     R@50:  67.52985119819641
Mean Now:  56.27910494804382  Best Mean Before:  56.34491 --------------------
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][0/17]	Loss 2.8467 (2.8467)	LossFusion 2.8467 (2.8467)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
Train Epoch: [114][16/17]	Loss 1.8425 (2.2917)	LossFusion 1.8425 (2.2917)	
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
R@10:  45.06299694379171     R@50:  67.4798031648
Mean Now:  56.27140005429585  Best Mean Before:  56.34491 --------------------
Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	
Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	

Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	
Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	
Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	
Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	
Train Epoch: [115][0/17]	Loss 2.8910 (2.8910)	LossFusion 2.8910 (2.8910)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
Train Epoch: [115][16/17]	Loss 1.8404 (2.2733)	LossFusion 1.8404 (2.2733)	
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
R@10:  45.0137585401535     R@50:  67.49517520268758
Mean Now:  56.25446687142054  Best Mean Before:  56.34491 --------------------
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][0/17]	Loss 2.8919 (2.8919)	LossFusion 2.8919 (2.8919)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
Train Epoch: [116][16/17]	Loss 1.8472 (2.2643)	LossFusion 1.8472 (2.2643)	
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
R@10:  45.06462117036184     R@50:  67.47881968816121
Mean Now:  56.27172042926152  Best Mean Before:  56.34491 --------------------
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][0/17]	Loss 2.7736 (2.7736)	LossFusion 2.7736 (2.7736)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
Train Epoch: [117][16/17]	Loss 1.8366 (2.2593)	LossFusion 1.8366 (2.2593)	
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
R@10:  45.098317662874855     R@50:  67.51204331715901
Mean Now:  56.30518049001694  Best Mean Before:  56.34491 --------------------
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][0/17]	Loss 2.8239 (2.8239)	LossFusion 2.8239 (2.8239)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
Train Epoch: [118][16/17]	Loss 1.7802 (2.2427)	LossFusion 1.7802 (2.2427)	
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
R@10:  45.081791281700134     R@50:  67.49632954597473
Mean Now:  56.28906041383743  Best Mean Before:  56.34491 --------------------
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][0/17]	Loss 2.7870 (2.7870)	LossFusion 2.7870 (2.7870)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
Train Epoch: [119][16/17]	Loss 1.8012 (2.2562)	LossFusion 1.8012 (2.2562)	
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
R@10:  45.11450330416361     R@50:  67.32948025067647
Mean Now:  56.221991777420044  Best Mean Before:  56.34491 --------------------
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][0/17]	Loss 2.8242 (2.8242)	LossFusion 2.8242 (2.8242)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
Train Epoch: [120][16/17]	Loss 1.8263 (2.2592)	LossFusion 1.8263 (2.2592)	
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
R@10:  45.130385955174766     R@50:  67.51268704732259
Mean Now:  56.32153650124867  Best Mean Before:  56.34491 --------------------
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][0/17]	Loss 2.7722 (2.7722)	LossFusion 2.7722 (2.7722)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
Train Epoch: [121][16/17]	Loss 1.7443 (2.2549)	LossFusion 1.7443 (2.2549)	
R@10:  R@10: 45.111462473869324      R@50:  45.11146247386932467.56415168444316 
    R@50:  67.56415168444316Mean Now: 
 56.33780707915624 Mean Now:  Best Mean Before:   56.3378070791562456.34491   Best Mean Before: -------------------- 
56.34491 --------------------
R@10:  45.111462473869324     R@50:  67.56415168444316
Mean Now:  56.33780707915624  Best Mean Before:  56.34491 --------------------
R@10:  45.111462473869324     R@50:  67.56415168444316
Mean Now:  56.33780707915624  Best Mean Before:  56.34491 --------------------
R@10:  45.111462473869324     R@50:  67.56415168444316
Mean Now:  56.33780707915624  Best Mean Before:  56.34491 --------------------
R@10:  45.111462473869324     R@50:  67.56415168444316
Mean Now:  56.33780707915624  Best Mean Before:  56.34491 --------------------
R@10:  45.111462473869324     R@50:  67.56415168444316
Mean Now:  56.33780707915624  Best Mean Before:  56.34491 --------------------
R@10:  45.111462473869324     R@50:  67.56415168444316
Mean Now:  56.33780707915624  Best Mean Before:  56.34491 --------------------
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][0/17]	Loss 2.8083 (2.8083)	LossFusion 2.8083 (2.8083)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
Train Epoch: [122][16/17]	Loss 1.7939 (2.2571)	LossFusion 1.7939 (2.2571)	
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
R@10:  45.1461007197698     R@50:  67.51345992088318
Mean Now:  56.32978032032649  Best Mean Before:  56.34491 --------------------
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][0/17]	Loss 2.7957 (2.7957)	LossFusion 2.7957 (2.7957)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
Train Epoch: [123][16/17]	Loss 1.8016 (2.2654)	LossFusion 1.8016 (2.2654)	
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
R@10:  45.1974352200826     R@50:  67.54505634307861
Best Mean Now:  56.37125 ******************************
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][0/17]	Loss 2.8358 (2.8358)	LossFusion 2.8358 (2.8358)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
Train Epoch: [124][16/17]	Loss 1.7960 (2.2479)	LossFusion 1.7960 (2.2479)	
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
R@10:  45.062865813573204     R@50:  67.44503180185954
Mean Now:  56.25394880771637  Best Mean Before:  56.37125 --------------------
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][0/17]	Loss 2.7759 (2.7759)	LossFusion 2.7759 (2.7759)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
Train Epoch: [125][16/17]	Loss 1.8028 (2.2581)	LossFusion 1.8028 (2.2581)	
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
R@10:  45.045224825541176     R@50:  67.56354769070943
Mean Now:  56.304386258125305  Best Mean Before:  56.37125 --------------------
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][0/17]	Loss 2.7592 (2.7592)	LossFusion 2.7592 (2.7592)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
Train Epoch: [126][16/17]	Loss 1.8162 (2.2420)	LossFusion 1.8162 (2.2420)	
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
R@10:  45.04651029904684     R@50:  67.72748629252116
Best Mean Now:  56.387 ******************************
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][0/17]	Loss 2.7806 (2.7806)	LossFusion 2.7806 (2.7806)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
Train Epoch: [127][16/17]	Loss 1.8391 (2.2317)	LossFusion 1.8391 (2.2317)	
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
R@10:  45.14613846937815     R@50:  67.56144960721333
Mean Now:  56.353794038295746  Best Mean Before:  56.387 --------------------
Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	
Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	
Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	
Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	
Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	
Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	

Train Epoch: [128][0/17]	Loss 2.8188 (2.8188)	LossFusion 2.8188 (2.8188)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
Train Epoch: [128][16/17]	Loss 1.8579 (2.2485)	LossFusion 1.8579 (2.2485)	
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
R@10:  45.21473447481791     R@50:  67.57844885190327
Best Mean Now:  56.39659 ******************************
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][0/17]	Loss 2.7765 (2.7765)	LossFusion 2.7765 (2.7765)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
Train Epoch: [129][16/17]	Loss 1.8008 (2.2260)	LossFusion 1.8008 (2.2260)	
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
R@10:  45.21524508794149     R@50:  67.64357089996338
Best Mean Now:  56.42941 ******************************
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][0/17]	Loss 2.7979 (2.7979)	LossFusion 2.7979 (2.7979)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
Train Epoch: [130][16/17]	Loss 1.8233 (2.2391)	LossFusion 1.8233 (2.2391)	
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
R@10:  45.097713669141136     R@50:  67.49508182207744
Mean Now:  56.29639774560928  Best Mean Before:  56.42941 --------------------
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][0/17]	Loss 2.8566 (2.8566)	LossFusion 2.8566 (2.8566)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
Train Epoch: [131][16/17]	Loss 1.8095 (2.2164)	LossFusion 1.8095 (2.2164)	
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
R@10:  45.16502618789673     R@50:  67.5125519434611
Mean Now:  56.33878906567892  Best Mean Before:  56.42941 --------------------
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][0/17]	Loss 2.8125 (2.8125)	LossFusion 2.8125 (2.8125)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
Train Epoch: [132][16/17]	Loss 1.8011 (2.2198)	LossFusion 1.8011 (2.2198)	
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
R@10:  45.1121042172114     R@50:  67.64185229937236
Mean Now:  56.376978258291885  Best Mean Before:  56.42941 --------------------
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][0/17]	Loss 2.7366 (2.7366)	LossFusion 2.7366 (2.7366)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
Train Epoch: [133][16/17]	Loss 1.8026 (2.2223)	LossFusion 1.8026 (2.2223)	
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
R@10:  45.22856076558431     R@50:  67.47744083404541
Mean Now:  56.353000799814865  Best Mean Before:  56.42941 --------------------
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
Train Epoch: [134][16/17]	Loss 1.8248 (2.2244)	LossFusion 1.8248 (2.2244)	
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
R@10:  45.15958627065023     R@50:  67.59244402249654
Mean Now:  56.37601514657338  Best Mean Before:  56.42941 --------------------
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][0/17]	Loss 2.7997 (2.7997)	LossFusion 2.7997 (2.7997)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
Train Epoch: [135][16/17]	Loss 1.7866 (2.2289)	LossFusion 1.7866 (2.2289)	
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
R@10:  45.044452945391335     R@50:  67.62610077857971
Mean Now:  56.33527686198552  Best Mean Before:  56.42941 --------------------
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][0/17]	Loss 2.8261 (2.8261)	LossFusion 2.8261 (2.8261)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
Train Epoch: [136][16/17]	Loss 1.7713 (2.2090)	LossFusion 1.7713 (2.2090)	
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
R@10:  45.07895906766256     R@50:  67.46121843655904
Mean Now:  56.270088752110794  Best Mean Before:  56.42941 --------------------
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][0/17]	Loss 2.7003 (2.7003)	LossFusion 2.7003 (2.7003)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
Train Epoch: [137][16/17]	Loss 1.7553 (2.2092)	LossFusion 1.7553 (2.2092)	
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
R@10:  45.02951304117838     R@50:  67.49392946561177
Mean Now:  56.26172125339508  Best Mean Before:  56.42941 --------------------
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][0/17]	Loss 2.7661 (2.7661)	LossFusion 2.7661 (2.7661)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
Train Epoch: [138][16/17]	Loss 1.7314 (2.1974)	LossFusion 1.7314 (2.1974)	
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
R@10:  45.1278954744339     R@50:  67.66064564387004
Mean Now:  56.39427055915197  Best Mean Before:  56.42941 --------------------
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][0/17]	Loss 2.7914 (2.7914)	LossFusion 2.7914 (2.7914)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
Train Epoch: [139][16/17]	Loss 1.8169 (2.2005)	LossFusion 1.8169 (2.2005)	
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
R@10:  45.014309883117676     R@50:  67.72813002268474
Mean Now:  56.37121995290121  Best Mean Before:  56.42941 --------------------
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][0/17]	Loss 2.7416 (2.7416)	LossFusion 2.7416 (2.7416)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
Train Epoch: [140][16/17]	Loss 1.7510 (2.1995)	LossFusion 1.7510 (2.1995)	
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
R@10:  44.89656786123911     R@50:  67.45891173680623
Mean Now:  56.177739799022675  Best Mean Before:  56.42941 --------------------
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][0/17]	Loss 2.7190 (2.7190)	LossFusion 2.7190 (2.7190)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
Train Epoch: [141][16/17]	Loss 1.7596 (2.1835)	LossFusion 1.7596 (2.1835)	
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
R@10:  45.09608944257101     R@50:  67.59193340937297
Mean Now:  56.344011425971985  Best Mean Before:  56.42941 --------------------
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][0/17]	Loss 2.7896 (2.7896)	LossFusion 2.7896 (2.7896)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
Train Epoch: [142][16/17]	Loss 1.7331 (2.1776)	LossFusion 1.7331 (2.1776)	
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
R@10:  45.14849980672201     R@50:  67.49474207560222
Mean Now:  56.32162094116211  Best Mean Before:  56.42941 --------------------
Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	
Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	
Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	
Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	

Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	
Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	
Train Epoch: [143][0/17]	Loss 2.7139 (2.7139)	LossFusion 2.7139 (2.7139)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
Train Epoch: [143][16/17]	Loss 1.7215 (2.1788)	LossFusion 1.7215 (2.1788)	
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
R@10:  45.2987809975942     R@50:  67.6790992418925
Best Mean Now:  56.48894 ******************************
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][0/17]	Loss 2.7197 (2.7197)	LossFusion 2.7197 (2.7197)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
Train Epoch: [144][16/17]	Loss 1.7700 (2.1684)	LossFusion 1.7700 (2.1684)	
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
R@10:  45.11193335056305     R@50:  67.52715309460957
Mean Now:  56.31954322258631  Best Mean Before:  56.48894 --------------------
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][0/17]	Loss 2.8033 (2.8033)	LossFusion 2.8033 (2.8033)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
Train Epoch: [145][16/17]	Loss 1.7763 (2.1687)	LossFusion 1.7763 (2.1687)	
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
R@10:  44.89335616429647     R@50:  67.45868921279907
Mean Now:  56.176022688547775  Best Mean Before:  56.48894 --------------------
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][0/17]	Loss 2.7389 (2.7389)	LossFusion 2.7389 (2.7389)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
Train Epoch: [146][16/17]	Loss 1.7139 (2.1638)	LossFusion 1.7139 (2.1638)	
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
R@10:  44.929882884025574     R@50:  67.49474207560222
Mean Now:  56.212312479813896  Best Mean Before:  56.48894 --------------------
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][0/17]	Loss 2.8281 (2.8281)	LossFusion 2.8281 (2.8281)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
Train Epoch: [147][16/17]	Loss 1.7332 (2.1667)	LossFusion 1.7332 (2.1667)	
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
R@10:  45.297157764434814     R@50:  67.31085777282715
Mean Now:  56.30400776863098  Best Mean Before:  56.48894 --------------------
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][0/17]	Loss 2.7192 (2.7192)	LossFusion 2.7192 (2.7192)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
Train Epoch: [148][16/17]	Loss 1.6592 (2.1417)	LossFusion 1.6592 (2.1417)	
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
R@10:  44.995554288228355     R@50:  67.4798031648
Mean Now:  56.237678726514176  Best Mean Before:  56.48894 --------------------
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][0/17]	Loss 2.6820 (2.6820)	LossFusion 2.6820 (2.6820)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
Train Epoch: [149][16/17]	Loss 1.7166 (2.1447)	LossFusion 1.7166 (2.1447)	
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
R@10:  45.195927222569786     R@50:  67.51015384991963
Mean Now:  56.353040536244706  Best Mean Before:  56.48894 --------------------
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][0/17]	Loss 2.7706 (2.7706)	LossFusion 2.7706 (2.7706)	
Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	
Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	
Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	
Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	
Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	

Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	
Train Epoch: [150][16/17]	Loss 1.7335 (2.1500)	LossFusion 1.7335 (2.1500)	
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
R@10:  44.81046199798584     R@50:  67.47653881708781
Mean Now:  56.14350040753683  Best Mean Before:  56.48894 --------------------
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][0/17]	Loss 2.6828 (2.6828)	LossFusion 2.6828 (2.6828)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
Train Epoch: [151][16/17]	Loss 1.7035 (2.1351)	LossFusion 1.7035 (2.1351)	
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
R@10:  44.74628468354543     R@50:  67.57741371790569
Mean Now:  56.161849200725555  Best Mean Before:  56.48894 --------------------
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][0/17]	Loss 2.6794 (2.6794)	LossFusion 2.6794 (2.6794)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
Train Epoch: [152][16/17]	Loss 1.6337 (2.1353)	LossFusion 1.6337 (2.1353)	
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
R@10:  44.8824405670166     R@50:  67.37190286318462
Mean Now:  56.12717171510061  Best Mean Before:  56.48894 --------------------
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][0/17]	Loss 2.6897 (2.6897)	LossFusion 2.6897 (2.6897)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
Train Epoch: [153][16/17]	Loss 1.6122 (2.1066)	LossFusion 1.6122 (2.1066)	
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
R@10:  44.88209982713064     R@50:  67.20603704452515
Mean Now:  56.044068435827896  Best Mean Before:  56.48894 --------------------
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][0/17]	Loss 2.6975 (2.6975)	LossFusion 2.6975 (2.6975)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
Train Epoch: [154][16/17]	Loss 1.6769 (2.1286)	LossFusion 1.6769 (2.1286)	
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
R@10:  44.99585727850596     R@50:  67.29569435119629
Mean Now:  56.14577581485112  Best Mean Before:  56.48894 --------------------
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][0/17]	Loss 2.6475 (2.6475)	LossFusion 2.6475 (2.6475)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
Train Epoch: [155][16/17]	Loss 1.6504 (2.0911)	LossFusion 1.6504 (2.0911)	
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
R@10:  45.08118728796641     R@50:  67.25950837135315
Mean Now:  56.170347829659775  Best Mean Before:  56.48894 --------------------
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][0/17]	Loss 2.6359 (2.6359)	LossFusion 2.6359 (2.6359)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
Train Epoch: [156][16/17]	Loss 1.6299 (2.0985)	LossFusion 1.6299 (2.0985)	
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.45624939600627
Mean Now:  56.11541370550792  Best Mean Before:  56.48894 --------------------
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][0/17]	Loss 2.6680 (2.6680)	LossFusion 2.6680 (2.6680)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
Train Epoch: [157][16/17]	Loss 1.7220 (2.0908)	LossFusion 1.7220 (2.0908)	
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
R@10:  44.84038154284159     R@50:  67.49170025189717
Mean Now:  56.166040897369385  Best Mean Before:  56.48894 --------------------
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][0/17]	Loss 2.6906 (2.6906)	LossFusion 2.6906 (2.6906)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
Train Epoch: [158][16/17]	Loss 1.6394 (2.1012)	LossFusion 1.6394 (2.1012)	
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894R@10:   --------------------
44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
R@10:  44.47731673717499     R@50:  67.26139783859253
Mean Now:  55.86935728788376  Best Mean Before:  56.48894 --------------------
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][0/17]	Loss 2.6446 (2.6446)	LossFusion 2.6446 (2.6446)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
Train Epoch: [159][16/17]	Loss 1.5649 (2.0799)	LossFusion 1.5649 (2.0799)	
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
R@10:  44.832088549931846     R@50:  67.11244980494182
Mean Now:  55.97226917743683  Best Mean Before:  56.48894 --------------------
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][0/17]	Loss 2.6126 (2.6126)	LossFusion 2.6126 (2.6126)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
Train Epoch: [160][16/17]	Loss 1.7065 (2.0595)	LossFusion 1.7065 (2.0595)	
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
R@10:  44.73066329956055     R@50:  67.19636718432109
Mean Now:  55.96351524194082  Best Mean Before:  56.48894 --------------------
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][0/17]	Loss 2.6138 (2.6138)	LossFusion 2.6138 (2.6138)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
Train Epoch: [161][16/17]	Loss 1.5722 (2.0586)	LossFusion 1.5722 (2.0586)	
R@10: R@10:   45.16476293404897 45.16476293404897    R@50:       R@50: 67.20055937767029 
67.20055937767029Mean Now: 
 56.182661155859634  Best Mean Before: Mean Now:   56.4889456.182661155859634  -------------------- Best Mean Before: 
 56.48894 --------------------
R@10:  45.16476293404897     R@50:  67.20055937767029
Mean Now:  56.182661155859634  Best Mean Before:  56.48894 --------------------
R@10:  45.16476293404897     R@50:  67.20055937767029
Mean Now:  56.182661155859634  Best Mean Before:  56.48894 --------------------
R@10:  45.16476293404897     R@50:  67.20055937767029
Mean Now:  56.182661155859634  Best Mean Before:  56.48894 --------------------
R@10:  45.16476293404897     R@50:  67.20055937767029
Mean Now:  56.182661155859634  Best Mean Before:  56.48894 --------------------
R@10:  45.16476293404897     R@50:  67.20055937767029
Mean Now:  56.182661155859634  Best Mean Before:  56.48894 --------------------
R@10:  45.16476293404897     R@50:  67.20055937767029
Mean Now:  56.182661155859634  Best Mean Before:  56.48894 --------------------
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][0/17]	Loss 2.6176 (2.6176)	LossFusion 2.6176 (2.6176)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
Train Epoch: [162][16/17]	Loss 1.6615 (2.0346)	LossFusion 1.6615 (2.0346)	
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
R@10:  44.930604100227356     R@50:  67.27818449338277
Mean Now:  56.10439429680506  Best Mean Before:  56.48894 --------------------
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][0/17]	Loss 2.6338 (2.6338)	LossFusion 2.6338 (2.6338)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
Train Epoch: [163][16/17]	Loss 1.6523 (2.0213)	LossFusion 1.6523 (2.0213)	
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
R@10:  44.75809137026469     R@50:  67.44384169578552
Mean Now:  56.1009665330251  Best Mean Before:  56.48894 --------------------
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][0/17]	Loss 2.6667 (2.6667)	LossFusion 2.6667 (2.6667)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
Train Epoch: [164][16/17]	Loss 1.6616 (2.0435)	LossFusion 1.6616 (2.0435)	
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
R@10:  44.50260798136393     R@50:  67.44584441184998
Mean Now:  55.97422619660695  Best Mean Before:  56.48894 --------------------
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][0/17]	Loss 2.5734 (2.5734)	LossFusion 2.5734 (2.5734)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
Train Epoch: [165][16/17]	Loss 1.5853 (2.0274)	LossFusion 1.5853 (2.0274)	
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
R@10:  44.69023048877716     R@50:  67.54076878229777
Mean Now:  56.11549963553747  Best Mean Before:  56.48894 --------------------
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][0/17]	Loss 2.5856 (2.5856)	LossFusion 2.5856 (2.5856)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	
Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	Train Epoch: [166][16/17]	Loss 1.5163 (2.0128)	LossFusion 1.5163 (2.0128)	

R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
R@10:  44.49499547481537     R@50:  67.21040209134419
Mean Now:  55.85269878307978  Best Mean Before:  56.48894 --------------------
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][0/17]	Loss 2.6222 (2.6222)	LossFusion 2.6222 (2.6222)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
Train Epoch: [167][16/17]	Loss 1.5679 (2.0123)	LossFusion 1.5679 (2.0123)	
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
R@10:  44.37472422917684     R@50:  66.99291666348775
Mean Now:  55.68382044633229  Best Mean Before:  56.48894 --------------------
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][0/17]	Loss 2.5601 (2.5601)	LossFusion 2.5601 (2.5601)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
Train Epoch: [168][16/17]	Loss 1.5138 (1.9994)	LossFusion 1.5138 (1.9994)	
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
R@10:  44.68244711558024     R@50:  66.5463387966156
Mean Now:  55.614392956097916  Best Mean Before:  56.48894 --------------------
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][0/17]	Loss 2.5271 (2.5271)	LossFusion 2.5271 (2.5271)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
Train Epoch: [169][16/17]	Loss 1.5805 (2.0006)	LossFusion 1.5805 (2.0006)	
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
R@10:  44.60780918598175     R@50:  66.70770645141602
Mean Now:  55.65775781869888  Best Mean Before:  56.48894 --------------------
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][0/17]	Loss 2.5555 (2.5555)	LossFusion 2.5555 (2.5555)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
Train Epoch: [170][16/17]	Loss 1.6026 (2.0039)	LossFusion 1.6026 (2.0039)	
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
R@10:  44.24451986948649     R@50:  66.75681273142497
Mean Now:  55.500666300455734  Best Mean Before:  56.48894 --------------------
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][0/17]	Loss 2.6700 (2.6700)	LossFusion 2.6700 (2.6700)	
Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	
Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	

Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	
Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	
Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	
Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	
Train Epoch: [171][16/17]	Loss 1.4394 (1.9911)	LossFusion 1.4394 (1.9911)	
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
R@10:  44.77457801500956     R@50:  67.13855663935344
Mean Now:  55.9565673271815  Best Mean Before:  56.48894 --------------------
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][0/17]	Loss 2.5387 (2.5387)	LossFusion 2.5387 (2.5387)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
Train Epoch: [172][16/17]	Loss 1.4674 (1.9479)	LossFusion 1.4674 (1.9479)	
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
R@10:  44.443019231160484     R@50:  67.09587176640828
Mean Now:  55.76944549878438  Best Mean Before:  56.48894 --------------------
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][0/17]	Loss 2.6583 (2.6583)	LossFusion 2.6583 (2.6583)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
Train Epoch: [173][16/17]	Loss 1.5755 (1.9762)	LossFusion 1.5755 (1.9762)	
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
R@10:  44.60948705673218     R@50:  67.02647805213928
Mean Now:  55.81798255443573  Best Mean Before:  56.48894 --------------------
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][0/17]	Loss 2.6009 (2.6009)	LossFusion 2.6009 (2.6009)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
Train Epoch: [174][16/17]	Loss 1.5400 (1.9602)	LossFusion 1.5400 (1.9602)	
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
R@10:  44.56066687901815     R@50:  67.15966860453288
Mean Now:  55.86016774177551  Best Mean Before:  56.48894 --------------------
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][0/17]	Loss 2.5306 (2.5306)	LossFusion 2.5306 (2.5306)	
Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	
Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	
Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	
Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	
Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	
Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	

Train Epoch: [175][16/17]	Loss 1.4360 (1.9234)	LossFusion 1.4360 (1.9234)	
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
R@10:  44.476451476415     R@50:  67.06508596738179
Mean Now:  55.77076872189839  Best Mean Before:  56.48894 --------------------
Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	
Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	
Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	

Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	
Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	
Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	
Train Epoch: [176][0/17]	Loss 2.4539 (2.4539)	LossFusion 2.4539 (2.4539)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
Train Epoch: [176][16/17]	Loss 1.4981 (1.9271)	LossFusion 1.4981 (1.9271)	
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
R@10:  44.4841722647349     R@50:  67.2649621963501
Mean Now:  55.874567230542496  Best Mean Before:  56.48894 --------------------
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][0/17]	Loss 2.5699 (2.5699)	LossFusion 2.5699 (2.5699)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
Train Epoch: [177][16/17]	Loss 1.4329 (1.9432)	LossFusion 1.4329 (1.9432)	
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
R@10:  44.049338499704994     R@50:  66.63323044776917
Mean Now:  55.341284473737076  Best Mean Before:  56.48894 --------------------
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][0/17]	Loss 2.5402 (2.5402)	LossFusion 2.5402 (2.5402)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
Train Epoch: [178][16/17]	Loss 1.4097 (1.9299)	LossFusion 1.4097 (1.9299)	
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
R@10:  43.87588302294413     R@50:  66.6256030400594
Mean Now:  55.25074303150177  Best Mean Before:  56.48894 --------------------
Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	
Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	

Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	
Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	
Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	
Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	
Train Epoch: [179][0/17]	Loss 2.6492 (2.6492)	LossFusion 2.6492 (2.6492)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
Train Epoch: [179][16/17]	Loss 1.4547 (1.9230)	LossFusion 1.4547 (1.9230)	
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
R@10:  44.00290151437124     R@50:  66.70925617218018
Mean Now:  55.35607884327571  Best Mean Before:  56.48894 --------------------
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][0/17]	Loss 2.5721 (2.5721)	LossFusion 2.5721 (2.5721)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
Train Epoch: [180][16/17]	Loss 1.4661 (1.8851)	LossFusion 1.4661 (1.8851)	
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
R@10:  44.179134567578636     R@50:  67.12455948193868
Mean Now:  55.65184702475865  Best Mean Before:  56.48894 --------------------
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][0/17]	Loss 2.3857 (2.3857)	LossFusion 2.3857 (2.3857)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
Train Epoch: [181][16/17]	Loss 1.4860 (1.8753)	LossFusion 1.4860 (1.8753)	
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
R@10:  44.34633751710256     R@50:  66.56424005826314
Mean Now:  55.455288787682846  Best Mean Before:  56.48894 --------------------
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][0/17]	Loss 2.5270 (2.5270)	LossFusion 2.5270 (2.5270)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
Train Epoch: [182][16/17]	Loss 1.3585 (1.8824)	LossFusion 1.3585 (1.8824)	
R@10:  43.95696818828583     R@50:  65.98023970921834
Mean Now:  54.96860394875208  Best Mean Before:  56.48894 --------------------
R@10:  43.95696818828583     R@50:  65.98023970921834
Mean Now:  54.96860394875208  Best Mean Before:  56.48894 --------------------
R@10:  43.95696818828583     R@50:  65.98023970921834
Mean Now:  54.96860394875208  Best Mean Before:  56.48894 --------------------
R@10:  R@10: 43.95696818828583      R@50:  43.9569681882858365.98023970921834 
    R@50:  65.98023970921834Mean Now: 
 54.96860394875208 Mean Now:  Best Mean Before:   54.9686039487520856.48894   Best Mean Before: -------------------- 
56.48894 --------------------
R@10:  43.95696818828583     R@50:  65.98023970921834
Mean Now:  54.96860394875208  Best Mean Before:  56.48894 --------------------
R@10:  43.95696818828583     R@50:  65.98023970921834
Mean Now:  54.96860394875208  Best Mean Before:  56.48894 --------------------
R@10:  43.95696818828583     R@50:  65.98023970921834
Mean Now:  54.96860394875208  Best Mean Before:  56.48894 --------------------
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][0/17]	Loss 2.5255 (2.5255)	LossFusion 2.5255 (2.5255)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
Train Epoch: [183][16/17]	Loss 1.3610 (1.8747)	LossFusion 1.3610 (1.8747)	
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
R@10:  43.724904457728066     R@50:  66.16510550181071
Mean Now:  54.945004979769394  Best Mean Before:  56.48894 --------------------
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][0/17]	Loss 2.5683 (2.5683)	LossFusion 2.5683 (2.5683)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
Train Epoch: [184][16/17]	Loss 1.3969 (1.8706)	LossFusion 1.3969 (1.8706)	
R@10:  44.193958242734276     R@50:  66.74619913101196
Mean Now:  55.47007868687312  Best Mean Before:  56.48894 --------------------
R@10:  44.193958242734276     R@50:  66.74619913101196
Mean Now:  55.47007868687312  Best Mean Before:  56.48894 --------------------
R@10:  44.193958242734276     R@50:  66.74619913101196
Mean Now:  55.47007868687312  Best Mean Before:  56.48894 --------------------
R@10:  44.193958242734276     R@50:  66.74619913101196
Mean Now:  55.47007868687312  Best Mean Before:  56.48894 --------------------
R@10:  44.193958242734276     R@50:  66.74619913101196
Mean Now:  55.47007868687312  Best Mean Before:  56.48894 --------------------
R@10:  44.193958242734276     R@50:  66.74619913101196
Mean Now:  55.47007868687312  Best Mean Before:  56.48894 --------------------
R@10: R@10:   44.193958242734276 44.193958242734276    R@50:       R@50: 66.74619913101196 
66.74619913101196
Mean Now:  Mean Now: 55.47007868687312  55.47007868687312 Best Mean Before:    Best Mean Before: 56.48894  56.48894-------------------- 
--------------------
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][0/17]	Loss 2.5013 (2.5013)	LossFusion 2.5013 (2.5013)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
Train Epoch: [185][16/17]	Loss 1.4346 (1.8410)	LossFusion 1.4346 (1.8410)	
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
R@10:  43.97207796573639     R@50:  66.82699521382649
Mean Now:  55.39953658978144  Best Mean Before:  56.48894 --------------------
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][0/17]	Loss 2.5508 (2.5508)	LossFusion 2.5508 (2.5508)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
Train Epoch: [186][16/17]	Loss 1.3915 (1.8309)	LossFusion 1.3915 (1.8309)	
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
R@10:  42.87931422392527     R@50:  66.3901964823405
Mean Now:  54.63475535313289  Best Mean Before:  56.48894 --------------------
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][0/17]	Loss 2.5148 (2.5148)	LossFusion 2.5148 (2.5148)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
Train Epoch: [187][16/17]	Loss 1.3349 (1.8385)	LossFusion 1.3349 (1.8385)	
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
R@10:  43.030147751172386     R@50:  65.83582758903503
Mean Now:  54.432987670103714  Best Mean Before:  56.48894 --------------------
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][0/17]	Loss 2.4980 (2.4980)	LossFusion 2.4980 (2.4980)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
Train Epoch: [188][16/17]	Loss 1.3383 (1.8045)	LossFusion 1.3383 (1.8045)	
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
R@10:  43.180051445961     R@50:  66.11416339874268
Mean Now:  54.64710742235184  Best Mean Before:  56.48894 --------------------
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][0/17]	Loss 2.6214 (2.6214)	LossFusion 2.6214 (2.6214)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
Train Epoch: [189][16/17]	Loss 1.3402 (1.8071)	LossFusion 1.3402 (1.8071)	
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
R@10:  43.181885282198586     R@50:  66.1170740922292
Mean Now:  54.6494796872139  Best Mean Before:  56.48894 --------------------
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][0/17]	Loss 2.4558 (2.4558)	LossFusion 2.4558 (2.4558)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
Train Epoch: [190][16/17]	Loss 1.2571 (1.7789)	LossFusion 1.2571 (1.7789)	
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
R@10:  44.076635440190636     R@50:  66.79411133130391
Mean Now:  55.43537338574727  Best Mean Before:  56.48894 --------------------
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][0/17]	Loss 2.3281 (2.3281)	LossFusion 2.3281 (2.3281)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
Train Epoch: [191][16/17]	Loss 1.2919 (1.7534)	LossFusion 1.2919 (1.7534)	
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
R@10:  44.07843053340912     R@50:  67.01110402743022
Mean Now:  55.54476728041967  Best Mean Before:  56.48894 --------------------
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][0/17]	Loss 2.3312 (2.3312)	LossFusion 2.3312 (2.3312)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
Train Epoch: [192][16/17]	Loss 1.2802 (1.7506)	LossFusion 1.2802 (1.7506)	
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
R@10:  43.94780695438385     R@50:  66.62909785906474
Mean Now:  55.288452406724296  Best Mean Before:  56.48894 --------------------
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][0/17]	Loss 2.4104 (2.4104)	LossFusion 2.4104 (2.4104)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
Train Epoch: [193][16/17]	Loss 1.2926 (1.7612)	LossFusion 1.2926 (1.7612)	
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
R@10:  43.6776469151179     R@50:  66.41524036725362
Mean Now:  55.04644364118576  Best Mean Before:  56.48894 --------------------
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][0/17]	Loss 2.3725 (2.3725)	LossFusion 2.3725 (2.3725)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
Train Epoch: [194][16/17]	Loss 1.2838 (1.7376)	LossFusion 1.2838 (1.7376)	
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
R@10:  43.62801512082418     R@50:  66.34604334831238
Mean Now:  54.98702923456828  Best Mean Before:  56.48894 --------------------
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][0/17]	Loss 2.3326 (2.3326)	LossFusion 2.3326 (2.3326)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
Train Epoch: [195][16/17]	Loss 1.2229 (1.7118)	LossFusion 1.2229 (1.7118)	
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
R@10:  43.927860260009766     R@50:  66.34840369224548
Mean Now:  55.138131976127625  Best Mean Before:  56.48894 --------------------
Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	
Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	
Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	
Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	
Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	

Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	
Train Epoch: [196][0/17]	Loss 2.3181 (2.3181)	LossFusion 2.3181 (2.3181)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
Train Epoch: [196][16/17]	Loss 1.2494 (1.6913)	LossFusion 1.2494 (1.6913)	
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
R@10:  43.428547183672585     R@50:  66.34922822316487
Mean Now:  54.88888770341873  Best Mean Before:  56.48894 --------------------
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][0/17]	Loss 2.4074 (2.4074)	LossFusion 2.4074 (2.4074)	
Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	
Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	
Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	
Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	
Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	
Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	

Train Epoch: [197][16/17]	Loss 1.2613 (1.7252)	LossFusion 1.2613 (1.7252)	
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
R@10:  43.250991900761925     R@50:  66.38280749320984
Mean Now:  54.816899696985885  Best Mean Before:  56.48894 --------------------
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][0/17]	Loss 2.3619 (2.3619)	LossFusion 2.3619 (2.3619)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
Train Epoch: [198][16/17]	Loss 1.2840 (1.7062)	LossFusion 1.2840 (1.7062)	
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
R@10:  43.396384517351784     R@50:  66.43161376317342
Mean Now:  54.913999140262604  Best Mean Before:  56.48894 --------------------
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][0/17]	Loss 2.2631 (2.2631)	LossFusion 2.2631 (2.2631)	
Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	

Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	
Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	
Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	
Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	
Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	
Train Epoch: [199][16/17]	Loss 1.2901 (1.6577)	LossFusion 1.2901 (1.6577)	
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
R@10:  43.323673804601036     R@50:  66.36437972386678
Mean Now:  54.8440267642339  Best Mean Before:  56.48894 --------------------
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][0/17]	Loss 2.3633 (2.3633)	LossFusion 2.3633 (2.3633)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
Train Epoch: [200][16/17]	Loss 1.2848 (1.6566)	LossFusion 1.2848 (1.6566)	
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
R@10:  43.061391512552895     R@50:  66.49358471234639
Mean Now:  54.777488112449646  Best Mean Before:  56.48894 --------------------
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][0/17]	Loss 2.3555 (2.3555)	LossFusion 2.3555 (2.3555)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
Train Epoch: [201][16/17]	Loss 1.2036 (1.6507)	LossFusion 1.2036 (1.6507)	
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
R@10:  42.88385013739268     R@50:  65.86965521176656
Mean Now:  54.37675267457962  Best Mean Before:  56.48894 --------------------
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][0/17]	Loss 2.3538 (2.3538)	LossFusion 2.3538 (2.3538)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
Train Epoch: [202][16/17]	Loss 1.2180 (1.6559)	LossFusion 1.2180 (1.6559)	
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
R@10:  42.41546789805094     R@50:  65.48237999280293
Mean Now:  53.94892394542694  Best Mean Before:  56.48894 --------------------
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][0/17]	Loss 2.4920 (2.4920)	LossFusion 2.4920 (2.4920)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
Train Epoch: [203][16/17]	Loss 1.2252 (1.6630)	LossFusion 1.2252 (1.6630)	
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
R@10:  41.54754281044006     R@50:  65.22869269053142
Mean Now:  53.38811775048574  Best Mean Before:  56.48894 --------------------
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][0/17]	Loss 2.3437 (2.3437)	LossFusion 2.3437 (2.3437)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
Train Epoch: [204][16/17]	Loss 1.2515 (1.6563)	LossFusion 1.2515 (1.6563)	
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
R@10:  42.294747630755104     R@50:  65.12708266576131
Mean Now:  53.71091514825821  Best Mean Before:  56.48894 --------------------
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][0/17]	Loss 2.1688 (2.1688)	LossFusion 2.1688 (2.1688)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
Train Epoch: [205][16/17]	Loss 1.0983 (1.6323)	LossFusion 1.0983 (1.6323)	
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
R@10:  42.11672147115072     R@50:  65.26182492574056
Mean Now:  53.68927319844563  Best Mean Before:  56.48894 --------------------
Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	
Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	
Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	
Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	

Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	
Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	
Train Epoch: [206][0/17]	Loss 2.3693 (2.3693)	LossFusion 2.3693 (2.3693)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
Train Epoch: [206][16/17]	Loss 1.2022 (1.6809)	LossFusion 1.2022 (1.6809)	
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
R@10:  42.0788178841273     R@50:  65.43253660202026
Mean Now:  53.75567724307378  Best Mean Before:  56.48894 --------------------
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][0/17]	Loss 2.3987 (2.3987)	LossFusion 2.3987 (2.3987)	
Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	
Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	
Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	
Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	
Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	

Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	
Train Epoch: [207][16/17]	Loss 1.2282 (1.6326)	LossFusion 1.2282 (1.6326)	
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
R@10:  42.48800377051035     R@50:  65.60888687769572
Mean Now:  54.04844532410304  Best Mean Before:  56.48894 --------------------
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][0/17]	Loss 2.2922 (2.2922)	LossFusion 2.2922 (2.2922)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
Train Epoch: [208][16/17]	Loss 1.2841 (1.6295)	LossFusion 1.2841 (1.6295)	
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
R@10:  42.22711126009623     R@50:  65.45952359835307
Mean Now:  53.843317429224655  Best Mean Before:  56.48894 --------------------
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][0/17]	Loss 2.3075 (2.3075)	LossFusion 2.3075 (2.3075)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
Train Epoch: [209][16/17]	Loss 1.1236 (1.6153)	LossFusion 1.1236 (1.6153)	
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
R@10:  42.128634452819824     R@50:  64.98414079348247
Mean Now:  53.556387623151146  Best Mean Before:  56.48894 --------------------
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][0/17]	Loss 2.3635 (2.3635)	LossFusion 2.3635 (2.3635)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
Train Epoch: [210][16/17]	Loss 1.2128 (1.6040)	LossFusion 1.2128 (1.6040)	
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
R@10:  41.93848371505737     R@50:  64.50904607772827
Mean Now:  53.22376489639282  Best Mean Before:  56.48894 --------------------
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][0/17]	Loss 2.1914 (2.1914)	LossFusion 2.1914 (2.1914)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
Train Epoch: [211][16/17]	Loss 1.1038 (1.5674)	LossFusion 1.1038 (1.5674)	
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
R@10:  41.7318860689799     R@50:  64.89456494649251
Mean Now:  53.313225507736206  Best Mean Before:  56.48894 --------------------
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][0/17]	Loss 2.2011 (2.2011)	LossFusion 2.2011 (2.2011)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
Train Epoch: [212][16/17]	Loss 1.0235 (1.5849)	LossFusion 1.0235 (1.5849)	
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
R@10:  42.56046712398529     R@50:  65.56882659594218
Mean Now:  54.06464685996374  Best Mean Before:  56.48894 --------------------
Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	
Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	
Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	

Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	
Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	
Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	
Train Epoch: [213][0/17]	Loss 2.1581 (2.1581)	LossFusion 2.1581 (2.1581)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
Train Epoch: [213][16/17]	Loss 1.1376 (1.5657)	LossFusion 1.1376 (1.5657)	
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
R@10:  43.02958349386851     R@50:  65.57122468948364
Mean Now:  54.30040409167607  Best Mean Before:  56.48894 --------------------
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][0/17]	Loss 2.1444 (2.1444)	LossFusion 2.1444 (2.1444)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
Train Epoch: [214][16/17]	Loss 1.1632 (1.5486)	LossFusion 1.1632 (1.5486)	
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
R@10:  42.06938544909159     R@50:  64.91954525311787
Mean Now:  53.494465351104736  Best Mean Before:  56.48894 --------------------
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][0/17]	Loss 2.2707 (2.2707)	LossFusion 2.2707 (2.2707)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
Train Epoch: [215][16/17]	Loss 1.1312 (1.5918)	LossFusion 1.1312 (1.5918)	
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
R@10:  41.81183079878489     R@50:  64.6803895632426
Mean Now:  53.24611018101375  Best Mean Before:  56.48894 --------------------
Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	

Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	
Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	
Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	
Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	
Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	
Train Epoch: [216][0/17]	Loss 2.1456 (2.1456)	LossFusion 2.1456 (2.1456)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
Train Epoch: [216][16/17]	Loss 1.0615 (1.5013)	LossFusion 1.0615 (1.5013)	
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
R@10:  42.34024981657664     R@50:  65.53220748901367
Mean Now:  53.93622865279515  Best Mean Before:  56.48894 --------------------
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][0/17]	Loss 2.1461 (2.1461)	LossFusion 2.1461 (2.1461)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
Train Epoch: [217][16/17]	Loss 1.0633 (1.5114)	LossFusion 1.0633 (1.5114)	
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
R@10:  42.709397276242576     R@50:  65.79671899477641
Mean Now:  54.25305813550949  Best Mean Before:  56.48894 --------------------
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][0/17]	Loss 2.1419 (2.1419)	LossFusion 2.1419 (2.1419)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
Train Epoch: [218][16/17]	Loss 1.0846 (1.4967)	LossFusion 1.0846 (1.4967)	
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
R@10:  42.71184901396433     R@50:  65.5540943145752
Mean Now:  54.13297166426976  Best Mean Before:  56.48894 --------------------
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][0/17]	Loss 2.1784 (2.1784)	LossFusion 2.1784 (2.1784)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
Train Epoch: [219][16/17]	Loss 1.0807 (1.4837)	LossFusion 1.0807 (1.4837)	
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
R@10:  42.31371283531189     R@50:  65.47012527783711
Mean Now:  53.8919190565745  Best Mean Before:  56.48894 --------------------
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][0/17]	Loss 1.9794 (1.9794)	LossFusion 1.9794 (1.9794)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
Train Epoch: [220][16/17]	Loss 1.0532 (1.4328)	LossFusion 1.0532 (1.4328)	
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
R@10:  41.699265440305076     R@50:  65.20797212918599
Mean Now:  53.45361878474553  Best Mean Before:  56.48894 --------------------
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][0/17]	Loss 2.1014 (2.1014)	LossFusion 2.1014 (2.1014)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
Train Epoch: [221][16/17]	Loss 1.0616 (1.4551)	LossFusion 1.0616 (1.4551)	
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
R@10:  41.768411795298256     R@50:  64.93900616963704
Mean Now:  53.35370898246765  Best Mean Before:  56.48894 --------------------
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][0/17]	Loss 2.0048 (2.0048)	LossFusion 2.0048 (2.0048)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
Train Epoch: [222][16/17]	Loss 1.0347 (1.4513)	LossFusion 1.0347 (1.4513)	
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
R@10:  41.832627852757774     R@50:  65.0877018769582
Mean Now:  53.46016486485799  Best Mean Before:  56.48894 --------------------
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][0/17]	Loss 2.0434 (2.0434)	LossFusion 2.0434 (2.0434)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
Train Epoch: [223][16/17]	Loss 1.0306 (1.4064)	LossFusion 1.0306 (1.4064)	
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
R@10:  41.890515883763634     R@50:  64.99914526939392
Mean Now:  53.44483057657878  Best Mean Before:  56.48894 --------------------
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][0/17]	Loss 1.9984 (1.9984)	LossFusion 1.9984 (1.9984)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
Train Epoch: [224][16/17]	Loss 0.9703 (1.3884)	LossFusion 0.9703 (1.3884)	
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
R@10:  41.84988935788473     R@50:  65.21708170572917
Mean Now:  53.533485531806946  Best Mean Before:  56.48894 --------------------
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][0/17]	Loss 1.9651 (1.9651)	LossFusion 1.9651 (1.9651)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
Train Epoch: [225][16/17]	Loss 0.9549 (1.4106)	LossFusion 0.9549 (1.4106)	
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
R@10:  42.10202991962433     R@50:  65.23819367090861
Mean Now:  53.67011179526647  Best Mean Before:  56.48894 --------------------
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][0/17]	Loss 1.9598 (1.9598)	LossFusion 1.9598 (1.9598)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
Train Epoch: [226][16/17]	Loss 0.9429 (1.3772)	LossFusion 0.9429 (1.3772)	
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
R@10:  41.446784138679504     R@50:  64.81920679410298
Mean Now:  53.13299546639124R@10:    Best Mean Before:  41.44678413867950456.48894      R@50: -------------------- 
64.81920679410298
Mean Now:  53.13299546639124  Best Mean Before:  56.48894 --------------------
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][0/17]	Loss 1.9775 (1.9775)	LossFusion 1.9775 (1.9775)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
Train Epoch: [227][16/17]	Loss 0.9973 (1.3950)	LossFusion 0.9973 (1.3950)	
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
R@10:  41.16210341453552     R@50:  64.28041855494182
Mean Now:  52.72126098473867  Best Mean Before:  56.48894 --------------------
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][0/17]	Loss 2.0218 (2.0218)	LossFusion 2.0218 (2.0218)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
Train Epoch: [228][16/17]	Loss 0.9866 (1.3746)	LossFusion 0.9866 (1.3746)	
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
R@10:  41.080873211224876     R@50:  64.552636941274
Mean Now:  52.816755076249436  Best Mean Before:  56.48894 --------------------
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][0/17]	Loss 2.0139 (2.0139)	LossFusion 2.0139 (2.0139)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
Train Epoch: [229][16/17]	Loss 0.9467 (1.3454)	LossFusion 0.9467 (1.3454)	
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
R@10:  41.828604539235435     R@50:  65.1184876759847
Mean Now:  53.47354610761006  Best Mean Before:  56.48894 --------------------
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][0/17]	Loss 1.9166 (1.9166)	LossFusion 1.9166 (1.9166)	
Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	
Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	
Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	
Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	

Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	
Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	
Train Epoch: [230][16/17]	Loss 0.9597 (1.3368)	LossFusion 0.9597 (1.3368)	
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
R@10:  41.88407063484192     R@50:  65.06569782892863
Mean Now:  53.47488423188528  Best Mean Before:  56.48894 --------------------
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][0/17]	Loss 1.9118 (1.9118)	LossFusion 1.9118 (1.9118)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
Train Epoch: [231][16/17]	Loss 0.9841 (1.3435)	LossFusion 0.9841 (1.3435)	
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
R@10:  41.13144874572754     R@50:  64.4990344842275
Mean Now:  52.815241614977516  Best Mean Before:  56.48894 --------------------
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][0/17]	Loss 1.9762 (1.9762)	LossFusion 1.9762 (1.9762)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
Train Epoch: [232][16/17]	Loss 0.8993 (1.3450)	LossFusion 0.8993 (1.3450)	
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
R@10:  41.26143058141073     R@50:  64.21250303586324
Mean Now:  52.73696680863698  Best Mean Before:  56.48894 --------------------
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][0/17]	Loss 1.8195 (1.8195)	LossFusion 1.8195 (1.8195)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
Train Epoch: [233][16/17]	Loss 0.9607 (1.3156)	LossFusion 0.9607 (1.3156)	
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
R@10:  41.50889019171397     R@50:  64.43166931470235
Mean Now:  52.97027975320816  Best Mean Before:  56.48894 --------------------
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][0/17]	Loss 1.8194 (1.8194)	LossFusion 1.8194 (1.8194)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
Train Epoch: [234][16/17]	Loss 0.9182 (1.3199)	LossFusion 0.9182 (1.3199)	
R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
R@10: R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
 41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
R@10:  41.34602745374044     R@50:  64.568297068278
Mean Now:  52.957162261009216  Best Mean Before:  56.48894 --------------------
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][0/17]	Loss 1.9002 (1.9002)	LossFusion 1.9002 (1.9002)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
Train Epoch: [235][16/17]	Loss 0.9516 (1.3034)	LossFusion 0.9516 (1.3034)	
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
R@10:  40.89719752470652     R@50:  64.67593312263489
Mean Now:  52.7865653236707  Best Mean Before:  56.48894 --------------------
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][0/17]	Loss 1.8953 (1.8953)	LossFusion 1.8953 (1.8953)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
Train Epoch: [236][16/17]	Loss 0.8973 (1.3139)	LossFusion 0.8973 (1.3139)	
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
R@10:  40.510448813438416     R@50:  63.483711083730064
Mean Now:  51.99707994858424  Best Mean Before:  56.48894 --------------------
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][0/17]	Loss 1.9175 (1.9175)	LossFusion 1.9175 (1.9175)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
Train Epoch: [237][16/17]	Loss 0.9142 (1.3008)	LossFusion 0.9142 (1.3008)	
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
R@10:  39.82598980267843     R@50:  63.261530796686806
Mean Now:  51.54376029968262  Best Mean Before:  56.48894 --------------------
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][0/17]	Loss 1.9088 (1.9088)	LossFusion 1.9088 (1.9088)	
Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	
Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	

Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	
Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	
Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	
Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	
Train Epoch: [238][16/17]	Loss 0.8820 (1.3124)	LossFusion 0.8820 (1.3124)	
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
R@10:  41.03403389453888     R@50:  64.19351696968079
Mean Now:  52.61377543210983  Best Mean Before:  56.48894 --------------------
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][0/17]	Loss 1.8641 (1.8641)	LossFusion 1.8641 (1.8641)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
Train Epoch: [239][16/17]	Loss 0.9402 (1.3140)	LossFusion 0.9402 (1.3140)	
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
R@10:  41.044547160466514     R@50:  64.30809895197551
Mean Now:  52.67632305622101  Best Mean Before:  56.48894 --------------------
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][0/17]	Loss 2.0069 (2.0069)	LossFusion 2.0069 (2.0069)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
Train Epoch: [240][16/17]	Loss 0.9711 (1.2979)	LossFusion 0.9711 (1.2979)	
R@10:  40.49046337604523     R@50:  64.15911118189494
Mean Now:  52.324787278970085  Best Mean Before:  56.48894 --------------------
R@10:  40.49046337604523     R@50:  64.15911118189494
Mean Now:  52.324787278970085  Best Mean Before:  56.48894 --------------------
R@10:  40.49046337604523     R@50:  64.15911118189494
Mean Now:  52.324787278970085  Best Mean Before:  56.48894 --------------------
R@10:  40.49046337604523     R@50:  64.15911118189494
Mean Now:  52.324787278970085  Best Mean Before:  56.48894 --------------------
R@10:  40.49046337604523     R@50:  64.15911118189494
Mean Now:  52.324787278970085  Best Mean Before:  56.48894 --------------------
R@10:  40.49046337604523     R@50:  64.15911118189494
Mean Now:  52.324787278970085  Best Mean Before:  56.48894 --------------------
R@10:  40.49046337604523     R@50:  R@10: 64.15911118189494 
40.49046337604523     R@50: Mean Now:   64.1591111818949452.324787278970085
  Best Mean Before:  56.48894Mean Now:   --------------------52.324787278970085
  Best Mean Before:  56.48894 --------------------
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][0/17]	Loss 1.8142 (1.8142)	LossFusion 1.8142 (1.8142)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
Train Epoch: [241][16/17]	Loss 0.9360 (1.2904)	LossFusion 0.9360 (1.2904)	
R@10:  40.11595547199249     R@50:  63.54779601097107
Mean Now:  51.83187574148178  Best Mean Before:  56.48894 --------------------
R@10:  40.11595547199249     R@50:  63.54779601097107
Mean Now:  51.83187574148178  Best Mean Before:  56.48894 --------------------
R@10:  40.11595547199249     R@50:  63.54779601097107
Mean Now:  51.83187574148178  Best Mean Before:  56.48894 --------------------
R@10:  40.11595547199249     R@50:  63.54779601097107
Mean Now:  51.83187574148178  Best Mean Before:  56.48894 --------------------
R@10:  40.11595547199249     R@50:  63.54779601097107
Mean Now:  51.83187574148178  Best Mean Before:  56.48894 --------------------
R@10:  40.11595547199249     R@50:  63.54779601097107
Mean Now:  51.83187574148178  Best Mean Before:  56.48894 --------------------
R@10: R@10:   40.11595547199249     R@50: 40.11595547199249  63.54779601097107    R@50: 
 63.54779601097107Mean Now: 
 51.83187574148178  Best Mean Before:  Mean Now: 56.48894  51.83187574148178-------------------- 
 Best Mean Before:  56.48894 --------------------
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][0/17]	Loss 1.8561 (1.8561)	LossFusion 1.8561 (1.8561)	
Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	
Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	

Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	
Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	
Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	
Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	
Train Epoch: [242][16/17]	Loss 0.9115 (1.3091)	LossFusion 0.9115 (1.3091)	
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
R@10:  39.462754130363464     R@50:  62.56294250488281
Mean Now:  51.01284831762314  Best Mean Before:  56.48894 --------------------
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][0/17]	Loss 1.9443 (1.9443)	LossFusion 1.9443 (1.9443)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
Train Epoch: [243][16/17]	Loss 0.8759 (1.3110)	LossFusion 0.8759 (1.3110)	
R@10:  41.065515081087746     R@50:  64.49493567148845
Mean Now:  52.7802253762881  Best Mean Before:  56.48894 --------------------
R@10:  41.065515081087746     R@50:  64.49493567148845
Mean Now:  52.7802253762881  Best Mean Before:  56.48894 --------------------
R@10:  41.065515081087746     R@50:  R@10: 64.49493567148845 
41.065515081087746 Mean Now:     R@50:   52.780225376288164.49493567148845 
 Best Mean Before:  56.48894Mean Now:   --------------------52.7802253762881
  Best Mean Before:  56.48894 --------------------
R@10:  41.065515081087746     R@50:  64.49493567148845
Mean Now:  52.7802253762881  Best Mean Before:  56.48894 --------------------
R@10:  41.065515081087746     R@50:  64.49493567148845
Mean Now:  52.7802253762881  Best Mean Before:  56.48894 --------------------
R@10:  41.065515081087746     R@50:  64.49493567148845
Mean Now:  52.7802253762881  Best Mean Before:  56.48894 --------------------
R@10:  41.065515081087746     R@50:  64.49493567148845
Mean Now:  52.7802253762881  Best Mean Before:  56.48894 --------------------
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][0/17]	Loss 1.9242 (1.9242)	LossFusion 1.9242 (1.9242)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
Train Epoch: [244][16/17]	Loss 0.8707 (1.3090)	LossFusion 0.8707 (1.3090)	
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
R@10:  41.2140945593516     R@50:  64.42455848058064
Mean Now:  52.819326519966125  Best Mean Before:  56.48894 --------------------
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][0/17]	Loss 1.9344 (1.9344)	LossFusion 1.9344 (1.9344)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
Train Epoch: [245][16/17]	Loss 0.8482 (1.2721)	LossFusion 0.8482 (1.2721)	
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
R@10:  39.732301235198975     R@50:  62.833486000696816
Mean Now:  51.28289361794789  Best Mean Before:  56.48894 --------------------
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][0/17]	Loss 1.8387 (1.8387)	LossFusion 1.8387 (1.8387)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
Train Epoch: [246][16/17]	Loss 0.9354 (1.2790)	LossFusion 0.9354 (1.2790)	
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
R@10:  39.8916761080424     R@50:  63.349467515945435
Mean Now:  51.62057181199391  Best Mean Before:  56.48894 --------------------
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][0/17]	Loss 1.7224 (1.7224)	LossFusion 1.7224 (1.7224)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
Train Epoch: [247][16/17]	Loss 0.9149 (1.2540)	LossFusion 0.9149 (1.2540)	
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
R@10:  41.37900173664093     R@50:  64.60561156272888
Mean Now:  52.992306649684906  Best Mean Before:  56.48894 --------------------
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][0/17]	Loss 1.8683 (1.8683)	LossFusion 1.8683 (1.8683)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
Train Epoch: [248][16/17]	Loss 0.8621 (1.2548)	LossFusion 0.8621 (1.2548)	
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
R@10:  40.91510077317556     R@50:  64.45746421813965
Mean Now:  52.68628249565761  Best Mean Before:  56.48894 --------------------
Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	
Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	
Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	
Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	
Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	

Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	
Train Epoch: [249][0/17]	Loss 1.8513 (1.8513)	LossFusion 1.8513 (1.8513)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
Train Epoch: [249][16/17]	Loss 0.8239 (1.2223)	LossFusion 0.8239 (1.2223)	
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
R@10:  40.44515689214071     R@50:  63.67184519767761
Mean Now:  52.058501044909164  Best Mean Before:  56.48894 --------------------
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][0/17]	Loss 1.6992 (1.6992)	LossFusion 1.6992 (1.6992)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
Train Epoch: [250][16/17]	Loss 0.8111 (1.2077)	LossFusion 0.8111 (1.2077)	
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
R@10:  40.16472200552622     R@50:  63.33116094271342
Mean Now:  51.74794147411982  Best Mean Before:  56.48894 --------------------
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][0/17]	Loss 1.7099 (1.7099)	LossFusion 1.7099 (1.7099)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
Train Epoch: [251][16/17]	Loss 0.8191 (1.2112)	LossFusion 0.8191 (1.2112)	
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
R@10:  40.74529012044271     R@50:  64.01607791582744
Mean Now:  52.38068401813507  Best Mean Before:  56.48894 --------------------
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][0/17]	Loss 1.6299 (1.6299)	LossFusion 1.6299 (1.6299)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
Train Epoch: [252][16/17]	Loss 0.9252 (1.2072)	LossFusion 0.9252 (1.2072)	
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
R@10:  40.598480900128685     R@50:  64.1566514968872
Mean Now:  52.37756619850795  Best Mean Before:  56.48894 --------------------
Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	
Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	
Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	

Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	
Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	
Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	
Train Epoch: [253][0/17]	Loss 1.7472 (1.7472)	LossFusion 1.7472 (1.7472)	
Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	
Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	
Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	
Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	
Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	
Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	

Train Epoch: [253][16/17]	Loss 0.8202 (1.1962)	LossFusion 0.8202 (1.1962)	
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
R@10:  40.064396460851036     R@50:  64.07562891642253
Mean Now:  52.07001268863678  Best Mean Before:  56.48894 --------------------
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][0/17]	Loss 1.6613 (1.6613)	LossFusion 1.6613 (1.6613)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
Train Epoch: [254][16/17]	Loss 0.8360 (1.1852)	LossFusion 0.8360 (1.1852)	
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
R@10:  40.04945755004883     R@50:  63.65265647570292
Mean Now:  51.85105701287587  Best Mean Before:  56.48894 --------------------
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][0/17]	Loss 1.6875 (1.6875)	LossFusion 1.6875 (1.6875)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
Train Epoch: [255][16/17]	Loss 0.7301 (1.1841)	LossFusion 0.7301 (1.1841)	
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
R@10:  40.644848346710205     R@50:  64.16520675023396
Mean Now:  52.405027548472084  Best Mean Before:  56.48894 --------------------
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][0/17]	Loss 1.7550 (1.7550)	LossFusion 1.7550 (1.7550)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
Train Epoch: [256][16/17]	Loss 0.8568 (1.1682)	LossFusion 0.8568 (1.1682)	
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
R@10:  40.13235171635946     R@50:  63.52601846059164
Mean Now:  51.82918508847555  Best Mean Before:  56.48894 --------------------
Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	

Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	
Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	
Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	
Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	
Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	
Train Epoch: [257][0/17]	Loss 1.7279 (1.7279)	LossFusion 1.7279 (1.7279)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
Train Epoch: [257][16/17]	Loss 0.8430 (1.1641)	LossFusion 0.8430 (1.1641)	
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
R@10:  40.59632917245229     R@50:  64.16845719019572
Mean Now:  52.382393181324005  Best Mean Before:  56.48894 --------------------
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][0/17]	Loss 1.5940 (1.5940)	LossFusion 1.5940 (1.5940)	
Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	
Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	
Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	
Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	

Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	
Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	
Train Epoch: [258][16/17]	Loss 0.8093 (1.1313)	LossFusion 0.8093 (1.1313)	
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
R@10:  40.39599597454071     R@50:  63.598754008611046
Mean Now:  51.99737499157588  Best Mean Before:  56.48894 --------------------
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][0/17]	Loss 1.6311 (1.6311)	LossFusion 1.6311 (1.6311)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
Train Epoch: [259][16/17]	Loss 0.7559 (1.1452)	LossFusion 0.7559 (1.1452)	
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
R@10:  40.450504422187805     R@50:  63.78522117932638
Mean Now:  52.117862800757095  Best Mean Before:  56.48894 --------------------
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][0/17]	Loss 1.7382 (1.7382)	LossFusion 1.7382 (1.7382)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
Train Epoch: [260][16/17]	Loss 0.8513 (1.1530)	LossFusion 0.8513 (1.1530)	
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
R@10:  40.13525942961375     R@50:  63.6041522026062
Mean Now:  51.86970581610997  Best Mean Before:  56.48894 --------------------
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][0/17]	Loss 1.6240 (1.6240)	LossFusion 1.6240 (1.6240)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
Train Epoch: [261][16/17]	Loss 0.8018 (1.1518)	LossFusion 0.8018 (1.1518)	
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
R@10:  40.1469349861145     R@50:  63.80019982655843
Mean Now:  51.97356740633647  Best Mean Before:  56.48894 --------------------
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][0/17]	Loss 1.6056 (1.6056)	LossFusion 1.6056 (1.6056)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
Train Epoch: [262][16/17]	Loss 0.8470 (1.1323)	LossFusion 0.8470 (1.1323)	
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
R@10:  40.44904907544454     R@50:  63.67093722025553
Mean Now:  52.05999314785004  Best Mean Before:  56.48894 --------------------
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][0/17]	Loss 1.5869 (1.5869)	LossFusion 1.5869 (1.5869)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
Train Epoch: [263][16/17]	Loss 0.7997 (1.1292)	LossFusion 0.7997 (1.1292)	
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
R@10:  40.111446380615234     R@50:  63.567797342936196
Mean Now:  51.83962186177571  Best Mean Before:  56.48894 --------------------
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][0/17]	Loss 1.7116 (1.7116)	LossFusion 1.7116 (1.7116)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
Train Epoch: [264][16/17]	Loss 0.8130 (1.1346)	LossFusion 0.8130 (1.1346)	
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
R@10:  40.27932981650034     R@50:  63.73473803202311
Mean Now:  52.00703392426173  Best Mean Before:  56.48894 --------------------
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][0/17]	Loss 1.6070 (1.6070)	LossFusion 1.6070 (1.6070)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
Train Epoch: [265][16/17]	Loss 0.8693 (1.1176)	LossFusion 0.8693 (1.1176)	
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
R@10:  40.24571279684702     R@50:  63.53555917739868
Mean Now:  51.89063598712285  Best Mean Before:  56.48894 --------------------
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][0/17]	Loss 1.5182 (1.5182)	LossFusion 1.5182 (1.5182)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
Train Epoch: [266][16/17]	Loss 0.8156 (1.1198)	LossFusion 0.8156 (1.1198)	
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
R@10:  40.07954696814219     R@50:  63.684688011805214
Mean Now:  51.8821174899737  Best Mean Before:  56.48894 --------------------
Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	
Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	
Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	

Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	
Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	
Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	
Train Epoch: [267][0/17]	Loss 1.7064 (1.7064)	LossFusion 1.7064 (1.7064)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
Train Epoch: [267][16/17]	Loss 0.8375 (1.1145)	LossFusion 0.8375 (1.1145)	
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
R@10:  40.380715330441795     R@50:  63.75430623690287
Mean Now:  52.06751078367233  Best Mean Before:  56.48894 --------------------
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][0/17]	Loss 1.5576 (1.5576)	LossFusion 1.5576 (1.5576)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
Train Epoch: [268][16/17]	Loss 0.7852 (1.1016)	LossFusion 0.7852 (1.1016)	
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
R@10:  40.14608363310496     R@50:  63.41923077901205
Mean Now:  51.7826572060585  Best Mean Before:  56.48894 --------------------
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][0/17]	Loss 1.5197 (1.5197)	LossFusion 1.5197 (1.5197)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
Train Epoch: [269][16/17]	Loss 0.7799 (1.1004)	LossFusion 0.7799 (1.1004)	
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
R@10:  40.19613365332285     R@50:  63.404460748036705
Mean Now:  51.80029720067978  Best Mean Before:  56.48894 --------------------
Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	
Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	
Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	
Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	
Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	
Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	

Train Epoch: [270][0/17]	Loss 1.5420 (1.5420)	LossFusion 1.5420 (1.5420)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
Train Epoch: [270][16/17]	Loss 0.8130 (1.1182)	LossFusion 0.8130 (1.1182)	
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
R@10:  40.328474839528404     R@50:  63.4043296178182
Mean Now:  51.8664022286733  Best Mean Before:  56.48894 --------------------
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][0/17]	Loss 1.6436 (1.6436)	LossFusion 1.6436 (1.6436)	
Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	
Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	

Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	
Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	
Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	
Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	
Train Epoch: [271][16/17]	Loss 0.7909 (1.1012)	LossFusion 0.7909 (1.1012)	
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
R@10:  40.245333313941956     R@50:  63.33531538645426
Mean Now:  51.790324350198105  Best Mean Before:  56.48894 --------------------
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][0/17]	Loss 1.5717 (1.5717)	LossFusion 1.5717 (1.5717)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
Train Epoch: [272][16/17]	Loss 0.7392 (1.0809)	LossFusion 0.7392 (1.0809)	
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
R@10:  40.062508980433144     R@50:  63.337451219558716
Mean Now:  51.699980099995926  Best Mean Before:  56.48894 --------------------
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][0/17]	Loss 1.5680 (1.5680)	LossFusion 1.5680 (1.5680)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
Train Epoch: [273][16/17]	Loss 0.7300 (1.0867)	LossFusion 0.7300 (1.0867)	
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
R@10:  40.21197954813639     R@50:  63.40223352114359
Mean Now:  51.80710653463999  Best Mean Before:  56.48894 --------------------
Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	
Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	
Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	
Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	
Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	

Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	
Train Epoch: [274][0/17]	Loss 1.6108 (1.6108)	LossFusion 1.6108 (1.6108)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
Train Epoch: [274][16/17]	Loss 0.7995 (1.1068)	LossFusion 0.7995 (1.1068)	
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
R@10:  40.29615819454193     R@50:  63.43991359074911
Mean Now:  51.86803589264552  Best Mean Before:  56.48894 --------------------
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][0/17]	Loss 1.5536 (1.5536)	LossFusion 1.5536 (1.5536)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
Train Epoch: [275][16/17]	Loss 0.7306 (1.0873)	LossFusion 0.7306 (1.0873)	
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
R@10:  40.09748796621958     R@50:  63.32109570503235
Mean Now:  51.70929183562596  Best Mean Before:  56.48894 --------------------
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][0/17]	Loss 1.4882 (1.4882)	LossFusion 1.4882 (1.4882)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
Train Epoch: [276][16/17]	Loss 0.8062 (1.0881)	LossFusion 0.8062 (1.0881)	
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
R@10:  40.21215041478475     R@50:  63.40077916781107
Mean Now:  51.80646479129791  Best Mean Before:  56.48894 --------------------
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][0/17]	Loss 1.5496 (1.5496)	LossFusion 1.5496 (1.5496)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
Train Epoch: [277][16/17]	Loss 0.7420 (1.0898)	LossFusion 0.7420 (1.0898)	
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
R@10:  40.13033211231232     R@50:  63.38574488957723
Mean Now:  51.75803850094478  Best Mean Before:  56.48894 --------------------
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][0/17]	Loss 1.5732 (1.5732)	LossFusion 1.5732 (1.5732)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
Train Epoch: [278][16/17]	Loss 0.8035 (1.0943)	LossFusion 0.8035 (1.0943)	
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
R@10:  40.09393652280172     R@50:  63.269629081090294
Mean Now:  51.68178280194601  Best Mean Before:  56.48894 --------------------
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][0/17]	Loss 1.5900 (1.5900)	LossFusion 1.5900 (1.5900)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
Train Epoch: [279][16/17]	Loss 0.7136 (1.0771)	LossFusion 0.7136 (1.0771)	
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
R@10:  40.27919868628184     R@50:  63.28658858935038
Mean Now:  51.78289363781611  Best Mean Before:  56.48894 --------------------
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][0/17]	Loss 1.5502 (1.5502)	LossFusion 1.5502 (1.5502)	
Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	
Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	
Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	

Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	
Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	
Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	
Train Epoch: [280][16/17]	Loss 0.7389 (1.0680)	LossFusion 0.7389 (1.0680)	
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
R@10:  40.013232827186584     R@50:  63.23657830556234
Mean Now:  51.624905566374466  Best Mean Before:  56.48894 --------------------
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][0/17]	Loss 1.5403 (1.5403)	LossFusion 1.5403 (1.5403)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
Train Epoch: [281][16/17]	Loss 0.8078 (1.0852)	LossFusion 0.8078 (1.0852)	
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
R@10:  40.030741691589355     R@50:  63.32096457481384
Mean Now:  51.6758531332016  Best Mean Before:  56.48894 --------------------
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][0/17]	Loss 1.5839 (1.5839)	LossFusion 1.5839 (1.5839)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
Train Epoch: [282][16/17]	Loss 0.6907 (1.0593)	LossFusion 0.6907 (1.0593)	
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
R@10:  39.981464544932045     R@50:  63.23705116907755
Mean Now:  51.6092578570048  Best Mean Before:  56.48894 --------------------
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	
Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	Train Epoch: [283][0/17]	Loss 1.5349 (1.5349)	LossFusion 1.5349 (1.5349)	

Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
Train Epoch: [283][16/17]	Loss 0.7905 (1.0886)	LossFusion 0.7905 (1.0886)	
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
R@10:  39.97825384140015     R@50:  63.236236572265625
Mean Now:  51.607245206832886  Best Mean Before:  56.48894 --------------------
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][0/17]	Loss 1.6044 (1.6044)	LossFusion 1.6044 (1.6044)	
Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	

Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	
Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	
Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	
Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	
Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	
Train Epoch: [284][16/17]	Loss 0.8189 (1.1089)	LossFusion 0.8189 (1.1089)	
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
R@10:  39.798168341318764     R@50:  63.13775976498922
Mean Now:  51.46796405315399  Best Mean Before:  56.48894 --------------------
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][0/17]	Loss 1.5353 (1.5353)	LossFusion 1.5353 (1.5353)	
Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	

Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	
Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	
Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	
Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	
Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	
Train Epoch: [285][16/17]	Loss 0.7130 (1.0716)	LossFusion 0.7130 (1.0716)	
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
R@10:  39.832767844200134     R@50:  63.21910619735718
Mean Now:  51.525937020778656  Best Mean Before:  56.48894 --------------------
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][0/17]	Loss 1.5866 (1.5866)	LossFusion 1.5866 (1.5866)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
Train Epoch: [286][16/17]	Loss 0.7796 (1.0771)	LossFusion 0.7796 (1.0771)	
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
R@10:  39.86710409323374     R@50:  63.17094564437866
Mean Now:  51.5190248688062  Best Mean Before:  56.48894 --------------------
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][0/17]	Loss 1.5737 (1.5737)	LossFusion 1.5737 (1.5737)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
Train Epoch: [287][16/17]	Loss 0.7197 (1.0623)	LossFusion 0.7197 (1.0623)	
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
R@10:  39.833107590675354     R@50:  63.15475900967916
Mean Now:  51.49393330017726  Best Mean Before:  56.48894 --------------------
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][0/17]	Loss 1.5456 (1.5456)	LossFusion 1.5456 (1.5456)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
Train Epoch: [288][16/17]	Loss 0.8075 (1.0707)	LossFusion 0.8075 (1.0707)	
R@10:  39.79829947153727     R@50:  63.204809029897056
Mean Now:  51.50155425071716  Best Mean Before:  56.48894 --------------------
R@10:  39.79829947153727     R@50:  63.204809029897056
Mean Now:  51.50155425071716  Best Mean Before:  56.48894 --------------------
R@10:  39.79829947153727     R@50:  63.204809029897056
Mean Now:  51.50155425071716  Best Mean Before:  56.48894 --------------------
R@10:  39.79829947153727 R@10:     R@50:   63.20480902989705639.79829947153727
     R@50:  63.204809029897056Mean Now: 
 51.50155425071716Mean Now:    Best Mean Before: 51.50155425071716  56.48894 Best Mean Before:   --------------------56.48894
 --------------------
R@10:  39.79829947153727     R@50:  63.204809029897056
Mean Now:  51.50155425071716  Best Mean Before:  56.48894 --------------------
R@10:  39.79829947153727     R@50:  63.204809029897056
Mean Now:  51.50155425071716  Best Mean Before:  56.48894 --------------------
R@10:  39.79829947153727     R@50:  63.204809029897056
Mean Now:  51.50155425071716  Best Mean Before:  56.48894 --------------------
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][0/17]	Loss 1.4969 (1.4969)	LossFusion 1.4969 (1.4969)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
Train Epoch: [289][16/17]	Loss 0.7393 (1.0642)	LossFusion 0.7393 (1.0642)	
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
R@10:  39.7978276014328     R@50:  63.287403186162315
Mean Now:  51.54261539379756  Best Mean Before:  56.48894 --------------------
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][0/17]	Loss 1.5991 (1.5991)	LossFusion 1.5991 (1.5991)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
Train Epoch: [290][16/17]	Loss 0.8619 (1.0876)	LossFusion 0.8619 (1.0876)	
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
R@10:  39.86470599969228     R@50:  63.25306495030721
Mean Now:  51.55888547499975  Best Mean Before:  56.48894 --------------------
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][0/17]	Loss 1.6105 (1.6105)	LossFusion 1.6105 (1.6105)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
Train Epoch: [291][16/17]	Loss 0.7598 (1.0739)	LossFusion 0.7598 (1.0739)	
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
R@10:  39.79701499144236     R@50:  63.23640743891398
Mean Now:  51.51671121517817  Best Mean Before:  56.48894 --------------------
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][0/17]	Loss 1.5898 (1.5898)	LossFusion 1.5898 (1.5898)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
Train Epoch: [292][16/17]	Loss 0.7568 (1.0885)	LossFusion 0.7568 (1.0885)	
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
R@10:  39.83006775379181     R@50:  63.20288379987081
Mean Now:  51.516475776831314  Best Mean Before:  56.48894 --------------------
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][0/17]	Loss 1.6641 (1.6641)	LossFusion 1.6641 (1.6641)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
Train Epoch: [293][16/17]	Loss 0.7491 (1.0876)	LossFusion 0.7491 (1.0876)	
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
R@10:  39.832126100858055     R@50:  63.22099566459656
Mean Now:  51.52656088272731  Best Mean Before:  56.48894 --------------------
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][0/17]	Loss 1.5454 (1.5454)	LossFusion 1.5454 (1.5454)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
Train Epoch: [294][16/17]	Loss 0.7038 (1.0715)	LossFusion 0.7038 (1.0715)	
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
R@10:  39.848482608795166     R@50:  63.22035392125448
Mean Now:  51.534418265024826  Best Mean Before:  56.48894 --------------------
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][0/17]	Loss 1.5769 (1.5769)	LossFusion 1.5769 (1.5769)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
Train Epoch: [295][16/17]	Loss 0.7429 (1.0758)	LossFusion 0.7429 (1.0758)	
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.20335467656454
Mean Now:  51.55030091603597  Best Mean Before:  56.48894 --------------------
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][0/17]	Loss 1.5984 (1.5984)	LossFusion 1.5984 (1.5984)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
Train Epoch: [296][16/17]	Loss 0.7702 (1.0593)	LossFusion 0.7702 (1.0593)	
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][0/17]	Loss 1.5806 (1.5806)	LossFusion 1.5806 (1.5806)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
Train Epoch: [297][16/17]	Loss 0.8394 (1.0749)	LossFusion 0.8394 (1.0749)	
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
R@10:  39.89724715550741     R@50:  63.18699916203817
Mean Now:  51.54212315877279  Best Mean Before:  56.48894 --------------------
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][0/17]	Loss 1.4512 (1.4512)	LossFusion 1.4512 (1.4512)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
Train Epoch: [298][16/17]	Loss 0.7651 (1.0678)	LossFusion 0.7651 (1.0678)	
R@10:  39.88072077433268     R@50:  R@10: 63.18699916203817 
39.88072077433268 Mean Now:     R@50:   51.53385996818542563.18699916203817 
 Best Mean Before:  56.48894 Mean Now: -------------------- 
51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][0/17]	Loss 1.5888 (1.5888)	LossFusion 1.5888 (1.5888)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
Train Epoch: [299][16/17]	Loss 0.8222 (1.0657)	LossFusion 0.8222 (1.0657)	
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
R@10:  39.88072077433268     R@50:  63.18699916203817
Mean Now:  51.533859968185425  Best Mean Before:  56.48894 --------------------
